{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b452e7",
   "metadata": {},
   "source": [
    "# Clock Drawing Segmentation Inference\n",
    "\n",
    "This notebook implements a comprehensive inference system for the segmentation of clock drawings from the Montreal Cognitive Assessment (MoCA) test. The notebook loads a pre-trained U-Net++ model with SE-ResNet50 backbone to perform multi-class segmentation of clock drawings, identifies their key components, and evaluates segmentation quality using multiple metrics.\n",
    "\n",
    "### Authors and Contact Information\n",
    "- **Diego Aldahir Tovar Ledesma** - diego.tovar@udem.edu\n",
    "- **Jorge Rodrigo GÃ³mez Mayo** - jorger.gomez@udem.edu\n",
    "\n",
    "**Organization:** Universidad de Monterrey  \n",
    "**First created:** April 2025\n",
    "\n",
    "### Project Overview\n",
    "This inference pipeline processes clock drawings to segment four key components:\n",
    "- The entire clock face\n",
    "- Numbers on the clock face\n",
    "- Clock hands (hour and minute)\n",
    "- Clock contour/outline\n",
    "\n",
    "The system evaluates segmentation quality using multiple metrics including standard IoU, relaxed IoU, Dice coefficient, boundary F1 score, Hausdorff distance, precision, and recall. These metrics provide a comprehensive assessment of segmentation performance for each component.\n",
    "\n",
    "### Technical Implementation\n",
    "- **Model Loading:** Pre-trained U-Net++ with SE-ResNet50 encoder\n",
    "- **Image Processing:** Maintains original aspect ratio with proper normalization\n",
    "- **Multiple Evaluation Metrics:** Standard and relaxed IoU, Dice, boundary F1, Hausdorff distance\n",
    "- **Output Format:** Transparent PNG masks for each component with evaluation reports\n",
    "\n",
    "### Usage Instructions\n",
    "The notebook requires:\n",
    "1. A trained model weights file (.pth)\n",
    "2. Input clock drawing images with \"_Background\" suffix\n",
    "3. Optional ground truth masks for evaluation\n",
    "\n",
    "Outputs include:\n",
    "- Segmented component masks as transparent PNGs\n",
    "- Visualization of all segmentation results\n",
    "- Comprehensive evaluation metrics in text format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf0ba0d",
   "metadata": {},
   "source": [
    "## Basic Setup and Configuration\n",
    "\n",
    "This code sets up the environment for clock drawing segmentation inference. It imports necessary libraries for image processing, deep learning, and visualization, and defines a configuration class with parameters for the segmentation model, including device selection for optimized performance on Apple Silicon hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import glob\n",
    "from torch import nn\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from skimage import measure\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Basic configuration\n",
    "class Config:\n",
    "    # Device for inference\n",
    "    DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    # Image size for the model\n",
    "    IMG_SIZE = 256\n",
    "    \n",
    "    # Model\n",
    "    ENCODER = 'se_resnet50'\n",
    "    ENCODER_WEIGHTS = 'imagenet'\n",
    "    CLASSES = ['entire', 'numbers', 'hands', 'contour']\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "# Verify device to ensure MPS is available\n",
    "print(f\"Processing device: {Config.DEVICE}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\" if hasattr(torch.backends, 'mps') else \"MPS not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe47ab7",
   "metadata": {},
   "source": [
    "## Model Architecture Selection\n",
    "\n",
    "This code defines a function to build the segmentation model with the specified architecture. It supports three architectures (Unet, Unet++, and Linknet) using the segmentation_models_pytorch library, and configures them with the parameters defined in the Config class. The default architecture is Unet++, which should match the architecture used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7919f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Builds the segmentation model.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: The constructed segmentation model with the specified architecture.\n",
    "    \"\"\"\n",
    "    model_type = \"Unet++\"\n",
    "    \n",
    "    if model_type == \"Unet\":\n",
    "        model = smp.Unet(\n",
    "            encoder_name=Config.ENCODER,\n",
    "            encoder_weights=Config.ENCODER_WEIGHTS,\n",
    "            classes=Config.NUM_CLASSES,\n",
    "            activation='sigmoid'\n",
    "        )\n",
    "    elif model_type == \"Unet++\":\n",
    "        model = smp.UnetPlusPlus(\n",
    "            encoder_name=Config.ENCODER,\n",
    "            encoder_weights=Config.ENCODER_WEIGHTS,\n",
    "            classes=Config.NUM_CLASSES,\n",
    "            activation='sigmoid'\n",
    "        )\n",
    "    elif model_type == \"Linknet\":\n",
    "        model = smp.Linknet(\n",
    "            encoder_name=Config.ENCODER,\n",
    "            encoder_weights=Config.ENCODER_WEIGHTS,\n",
    "            classes=Config.NUM_CLASSES,\n",
    "            activation='sigmoid'\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Model type not supported: {model_type}\")\n",
    "    \n",
    "    print(f\"Model built: {model_type} with encoder {Config.ENCODER}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa5f2a3",
   "metadata": {},
   "source": [
    "## Image Segmentation and Evaluation Functions\n",
    "\n",
    "This code provides comprehensive functions for evaluating segmentation quality with multiple metrics, and implements an image prediction pipeline. The evaluation metrics include IoU (Intersection over Union), relaxed IoU, Dice coefficient, boundary F1 score, Hausdorff distance, precision, and recall. The prediction function loads a trained model, processes input images, generates segmentation masks, saves them with transparency, and calculates performance metrics when ground truth is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred_mask, gt_mask):\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) between prediction and ground truth masks.\n",
    "    \n",
    "    Args:\n",
    "        pred_mask (numpy.ndarray): Predicted binary mask.\n",
    "        gt_mask (numpy.ndarray): Ground truth binary mask.\n",
    "        \n",
    "    Returns:\n",
    "        float: IoU score between 0 and 1.\n",
    "    \"\"\"\n",
    "    pred_mask = (pred_mask > 0).astype(np.uint8)\n",
    "    gt_mask = (gt_mask > 0).astype(np.uint8)\n",
    "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def calculate_relaxed_iou(pred_mask, gt_mask, tolerance=2):\n",
    "    \"\"\"\n",
    "    Calculates a relaxed IoU with dilated ground truth to be more forgiving on boundaries.\n",
    "    \n",
    "    Args:\n",
    "        pred_mask (numpy.ndarray): Predicted binary mask.\n",
    "        gt_mask (numpy.ndarray): Ground truth binary mask.\n",
    "        tolerance (int, optional): Dilation size for relaxed boundaries. Defaults to 2.\n",
    "        \n",
    "    Returns:\n",
    "        float: Relaxed IoU score between 0 and 1.\n",
    "    \"\"\"\n",
    "    pred_mask = (pred_mask > 0).astype(np.uint8)\n",
    "    gt_mask = (gt_mask > 0).astype(np.uint8)\n",
    "    kernel = np.ones((tolerance, tolerance), np.uint8)\n",
    "    gt_dilated = cv2.dilate(gt_mask, kernel, iterations=1)\n",
    "    intersection = np.logical_and(pred_mask, gt_dilated).sum()\n",
    "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def calculate_dice(pred_mask, gt_mask):\n",
    "    \"\"\"\n",
    "    Calculates the Dice coefficient (F1 score for segmentation).\n",
    "    \n",
    "    Args:\n",
    "        pred_mask (numpy.ndarray): Predicted binary mask.\n",
    "        gt_mask (numpy.ndarray): Ground truth binary mask.\n",
    "        \n",
    "    Returns:\n",
    "        float: Dice coefficient between 0 and 1.\n",
    "    \"\"\"\n",
    "    pred_mask = (pred_mask > 0).astype(np.uint8)\n",
    "    gt_mask = (gt_mask > 0).astype(np.uint8)\n",
    "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "    total = pred_mask.sum() + gt_mask.sum()\n",
    "    if total == 0:\n",
    "        return 1.0\n",
    "    return (2.0 * intersection) / total\n",
    "\n",
    "\n",
    "def calculate_boundary_f1(pred_mask, gt_mask):\n",
    "    \"\"\"\n",
    "    Calculates the boundary F1 score, focusing on contour accuracy.\n",
    "    \n",
    "    Args:\n",
    "        pred_mask (numpy.ndarray): Predicted binary mask.\n",
    "        gt_mask (numpy.ndarray): Ground truth binary mask.\n",
    "        \n",
    "    Returns:\n",
    "        float: Boundary F1 score between 0 and 1.\n",
    "    \"\"\"\n",
    "    pred_contours = measure.find_contours(pred_mask, 0.5)\n",
    "    gt_contours = measure.find_contours(gt_mask, 0.5)\n",
    "    if not pred_contours or not gt_contours:\n",
    "        return 0.0\n",
    "    pred_points = np.vstack(pred_contours)\n",
    "    gt_points = np.vstack(gt_contours)\n",
    "    dists_pred_to_gt = np.min(np.linalg.norm(pred_points[:, None] - gt_points[None], axis=2), axis=1)\n",
    "    dists_gt_to_pred = np.min(np.linalg.norm(gt_points[:, None] - pred_points[None], axis=2), axis=1)\n",
    "    precision = np.mean(dists_pred_to_gt < 2.0)\n",
    "    recall = np.mean(dists_gt_to_pred < 2.0)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "def calculate_hausdorff(pred_mask, gt_mask):\n",
    "    \"\"\"\n",
    "    Calculates the Hausdorff distance between prediction and ground truth masks.\n",
    "    \n",
    "    Args:\n",
    "        pred_mask (numpy.ndarray): Predicted binary mask.\n",
    "        gt_mask (numpy.ndarray): Ground truth binary mask.\n",
    "        \n",
    "    Returns:\n",
    "        float: Hausdorff distance (lower is better).\n",
    "    \"\"\"\n",
    "    pred_points = np.argwhere(pred_mask > 0)\n",
    "    gt_points = np.argwhere(gt_mask > 0)\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0:\n",
    "        return float('inf')\n",
    "    hd1 = directed_hausdorff(pred_points, gt_points)[0]\n",
    "    hd2 = directed_hausdorff(gt_points, pred_points)[0]\n",
    "    return max(hd1, hd2)\n",
    "\n",
    "\n",
    "def calculate_precision_recall(pred_mask, gt_mask):\n",
    "    \"\"\"\n",
    "    Calculates precision and recall metrics.\n",
    "    \n",
    "    Args:\n",
    "        pred_mask (numpy.ndarray): Predicted binary mask.\n",
    "        gt_mask (numpy.ndarray): Ground truth binary mask.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (precision, recall) scores between 0 and 1.\n",
    "    \"\"\"\n",
    "    pred_flat = pred_mask.flatten()\n",
    "    gt_flat = gt_mask.flatten()\n",
    "    precision = precision_score(gt_flat, pred_flat, zero_division=0)\n",
    "    recall = recall_score(gt_flat, pred_flat, zero_division=0)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def predict_image(model_path, image_path, output_dir, gt_dir=None):\n",
    "    \"\"\"\n",
    "    Processes an image through the segmentation model and evaluates results.\n",
    "    \n",
    "    This function loads a trained model, generates segmentation masks for an input image,\n",
    "    saves the results, and calculates evaluation metrics when ground truth is available.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model weights.\n",
    "        image_path (str): Path to the input image.\n",
    "        output_dir (str): Directory to save prediction outputs.\n",
    "        gt_dir (str, optional): Directory containing ground truth masks. Defaults to None.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Loading model...\")\n",
    "    model = build_model()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=Config.DEVICE))\n",
    "    model.to(Config.DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"ERROR: Could not read image: {image_path}\")\n",
    "        return\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transform = A.Compose([\n",
    "        A.Resize(Config.IMG_SIZE, Config.IMG_SIZE, interpolation=cv2.INTER_NEAREST),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    transformed = transform(image=image_rgb)\n",
    "    image_tensor = transformed['image'].unsqueeze(0).to(Config.DEVICE)\n",
    "\n",
    "    print(\"Generating predictions...\")\n",
    "    with torch.no_grad():\n",
    "        pred = model(image_tensor)\n",
    "\n",
    "    pred_np = pred.squeeze().cpu().numpy()\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "    gt_masks = {}\n",
    "    if gt_dir:\n",
    "        img_folder = os.path.dirname(image_path)\n",
    "        img_name = os.path.basename(image_path).split('_Background')[0]\n",
    "\n",
    "        for class_name in Config.CLASSES:\n",
    "            gt_path = os.path.join(img_folder, f\"{img_name}_{class_name}.*\")\n",
    "            gt_files = glob.glob(gt_path)\n",
    "            if gt_files:\n",
    "                gt_mask = cv2.imread(gt_files[0], cv2.IMREAD_UNCHANGED)\n",
    "                if gt_mask is not None:\n",
    "                    if gt_mask.shape[-1] == 4:\n",
    "                        gt_masks[class_name] = gt_mask[:, :, 3] > 127\n",
    "                    else:\n",
    "                        gt_gray = cv2.cvtColor(gt_mask, cv2.COLOR_BGR2GRAY)\n",
    "                        gt_masks[class_name] = gt_gray > 127\n",
    "\n",
    "    fig, axes = plt.subplots(1, Config.NUM_CLASSES + 1, figsize=(5*(Config.NUM_CLASSES + 1), 5))\n",
    "    axes[0].imshow(image_rgb)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "    iou_file = open(os.path.join(output_dir, f\"{base_name}_iou_scores.txt\"), 'w')\n",
    "    iou_file.write(\"Evaluation metrics by class:\\n\")\n",
    "    iou_file.write(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "    tolerancias = [2, 3, 5]\n",
    "\n",
    "    for i, class_name in enumerate(Config.CLASSES):\n",
    "        pred_class = cv2.resize(pred_np[i], (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "        pred_binary = (pred_class > 0.5).astype(np.uint8) * 255\n",
    "        axes[i+1].imshow(pred_binary, cmap='gray')\n",
    "        axes[i+1].set_title(f'Mask: {class_name}')\n",
    "        axes[i+1].axis('off')\n",
    "\n",
    "        rgba = np.zeros((h, w, 4), dtype=np.uint8)\n",
    "        rgba[..., 0:3] = 255\n",
    "        rgba[..., 3] = pred_binary\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}_{class_name}.png\")\n",
    "        cv2.imwrite(output_path, rgba)\n",
    "        print(f\"Mask saved: {output_path}\")\n",
    "\n",
    "        if class_name in gt_masks:\n",
    "            pred_mask_bin = pred_class > 0.5\n",
    "            gt_mask = gt_masks[class_name]\n",
    "\n",
    "            iou_score = calculate_iou(pred_mask_bin, gt_mask)\n",
    "            relaxed_iou_scores = {tol: calculate_relaxed_iou(pred_mask_bin, gt_mask, tolerance=tol) for tol in tolerancias}\n",
    "            dice_score = calculate_dice(pred_mask_bin, gt_mask)\n",
    "            bf1_score = calculate_boundary_f1(pred_mask_bin, gt_mask)\n",
    "            hausdorff = calculate_hausdorff(pred_mask_bin, gt_mask)\n",
    "            precision, recall = calculate_precision_recall(pred_mask_bin, gt_mask)\n",
    "\n",
    "            iou_file.write(f\"{class_name}:\\n\")\n",
    "            iou_file.write(f\"  - Standard IoU: {iou_score:.4f}\\n\")\n",
    "            for tol in tolerancias:\n",
    "                iou_file.write(f\"  - Relaxed IoU (tol={tol}px): {relaxed_iou_scores[tol]:.4f}\\n\")\n",
    "            iou_file.write(f\"  - Dice coefficient: {dice_score:.4f}\\n\")\n",
    "            iou_file.write(f\"  - Boundary F1 (tol=2px): {bf1_score:.4f}\\n\")\n",
    "            iou_file.write(f\"  - Hausdorff distance: {hausdorff:.2f}\\n\")\n",
    "            iou_file.write(f\"  - Precision: {precision:.4f}\\n\")\n",
    "            iou_file.write(f\"  - Recall: {recall:.4f}\\n\\n\")\n",
    "\n",
    "            print(f\"{class_name} -> IoU: {iou_score:.4f}, Dice: {dice_score:.4f}, BF1: {bf1_score:.4f}, HD: {hausdorff:.2f}, Prec: {precision:.4f}, Rec: {recall:.4f}\")\n",
    "\n",
    "    iou_file.close()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{base_name}_all_masks.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7453cd",
   "metadata": {},
   "source": [
    "## Model Inference and Evaluation\n",
    "\n",
    "This code executes the segmentation model on a specific image, evaluates its performance against ground truth masks, and saves the results. It first locates the model weights, identifies the input image with a \"_Background\" suffix, and then runs the prediction pipeline to generate segmentation masks and calculate various performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fba5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths\n",
    "model_path = '/Users/diegotovar/Developer/Tesis/UNet/checkpoints/v6/best_model.pth'\n",
    "img_folder_path = '/Users/diegotovar/Pictures/MoCA/Images Mask Files/IC00627P01E23PD00011_221130_2'\n",
    "output_dir = '/Users/diegotovar/Developer/Tesis/UNet/results'\n",
    "\n",
    "# Search for image with \"_Background\" suffix\n",
    "input_images = glob.glob(os.path.join(img_folder_path, \"*_Background*\"))\n",
    "\n",
    "if not input_images:\n",
    "    print(f\"ERROR: No image with '_Background' suffix found in {img_folder_path}\")\n",
    "else:\n",
    "    # Take the first image found\n",
    "    image_path = input_images[0]\n",
    "    print(f\"Input image: {os.path.basename(image_path)}\")\n",
    "    \n",
    "    # Ground truth directory is the same as the image directory\n",
    "    gt_dir = img_folder_path\n",
    "    \n",
    "    # Run prediction with IoU calculation\n",
    "    predictions = predict_image(model_path, image_path, output_dir, gt_dir)\n",
    "    \n",
    "    print(\"\\nResults saved in:\", output_dir)\n",
    "    print(\"Metrics file:\", os.path.join(output_dir, os.path.basename(image_path).split('.')[0] + \"_iou_scores.txt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
