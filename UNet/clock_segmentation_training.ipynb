{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32a33989",
   "metadata": {},
   "source": [
    "# Clock Drawing Segmentation with Deep Learning in the MoCA Test\n",
    "\n",
    "This notebook implements an advanced segmentation system for clock drawings in the Montreal Cognitive Assessment (MoCA) test using deep learning techniques. The system employs a U-Net++ architecture with SE-ResNet50 backbone to segment different components of clock drawings (entire clock, numbers, hands, and contour) to assist in the automated analysis for cognitive impairment detection.\n",
    "\n",
    "### Authors and Contact Information\n",
    "- **Diego Aldahir Tovar Ledesma** - diego.tovar@udem.edu\n",
    "- **Jorge Rodrigo GÃ³mez Mayo** - jorger.gomez@udem.edu\n",
    "\n",
    "**Organization:** Universidad de Monterrey  \n",
    "**First created:** April 2025\n",
    "\n",
    "### Project Overview\n",
    "This segmentation model is designed to identify four key components in clock drawings:\n",
    "- The entire clock face\n",
    "- Numbers on the clock face\n",
    "- Clock hands (hour and minute)\n",
    "- Clock contour/outline\n",
    "\n",
    "By accurately segmenting these elements, the system provides objective measurements that can help medical professionals detect early signs of cognitive impairment, particularly in conditions like Parkinson's disease and dementia.\n",
    "\n",
    "### Technical Implementation\n",
    "- **Architecture:** U-Net++ with SE-ResNet50 encoder (pre-trained on ImageNet)\n",
    "- **Loss Function:** Combined Dice Loss and Focal Loss with class weighting\n",
    "- **Data Processing:** Enhanced thin line detection for clock hands and contours\n",
    "- **Performance Metrics:** IoU (Intersection over Union) for each component\n",
    "\n",
    "### Usage Instructions\n",
    "The notebook is structured in sequential sections covering data loading, preprocessing, model definition, training, and evaluation. To run the entire pipeline, execute each cell in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c4be5a",
   "metadata": {},
   "source": [
    "## Initial Setup and Imports\n",
    "\n",
    "This code imports necessary libraries for the project, including data manipulation, image processing, deep learning frameworks, and visualization tools. It also sets up a reproducibility function to ensure consistent results across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# PyTorch and deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\n",
    "    Sets random seeds for reproducibility across all random number generators.\n",
    "    \n",
    "    Args:\n",
    "        seed (int, optional): The seed value to use. Defaults to 42.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Note:\n",
    "        This function sets seeds for Python's random module, NumPy, PyTorch CPU and\n",
    "        GPU operations, and configures CUDA backend for deterministic behavior.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Initialize seed for reproducibility\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c3661",
   "metadata": {},
   "source": [
    "## Configuration Setup and Device Checking\n",
    "\n",
    "This code sets up a version-controlled checkpoint system and defines configuration parameters for the neural network training process, including device selection, training hyperparameters, model architecture, and dataset paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_checkpoint_version(base_dir='checkpoints'):\n",
    "    \"\"\"\n",
    "    Creates a new versioned directory for saving model checkpoints.\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str, optional): Base directory for all checkpoint versions. \n",
    "            Defaults to 'checkpoints'.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the newly created checkpoint directory with incremented version.\n",
    "    \n",
    "    Note:\n",
    "        This function automatically increments version numbers (v1, v2, etc.)\n",
    "        based on existing directories.\n",
    "    \"\"\"\n",
    "    os.makedirs(base_dir, exist_ok=True)  # Create base folder if it doesn't exist\n",
    "\n",
    "    version_pattern = re.compile(r'^v(\\d+)$')\n",
    "    existing_versions = []\n",
    "\n",
    "    for name in os.listdir(base_dir):\n",
    "        match = version_pattern.match(name)\n",
    "        if match:\n",
    "            existing_versions.append(int(match.group(1)))\n",
    "\n",
    "    next_version = max(existing_versions, default=0) + 1\n",
    "    new_path = os.path.join(base_dir, f'v{next_version}')\n",
    "    os.makedirs(new_path, exist_ok=True)  # Create the new folder\n",
    "\n",
    "    return new_path\n",
    "\n",
    "# Create checkpoint path dynamically before defining the class\n",
    "DYNAMIC_CHECKPOINT_PATH = get_next_checkpoint_version()\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration class containing all parameters for model training and evaluation.\n",
    "    \n",
    "    Attributes:\n",
    "        DEVICE (torch.device): Computing device (CPU, MPS for Apple Silicon, or CUDA).\n",
    "        EPOCHS (int): Number of training epochs.\n",
    "        BATCH_SIZE (int): Batch size for training.\n",
    "        LEARNING_RATE (float): Learning rate for optimizer.\n",
    "        IMG_SIZE (int): Target image size for resizing.\n",
    "        ENCODER (str): Backbone encoder architecture.\n",
    "        ENCODER_WEIGHTS (str): Pre-trained weights for encoder.\n",
    "        CLASSES (list): List of segmentation class names.\n",
    "        NUM_CLASSES (int): Number of segmentation classes.\n",
    "        DATASET_PATH (str): Path to training dataset.\n",
    "        TEST_PATH (str): Path to test dataset.\n",
    "        CHECKPOINT_PATH (str): Path to save model checkpoints.\n",
    "        PATIENCE (int): Number of epochs with no improvement for early stopping.\n",
    "        VAL_FREQUENCY (int): Validation frequency in epochs.\n",
    "        SAVE_FREQUENCY (int): Model saving frequency in epochs.\n",
    "        VISUALIZATION_FREQUENCY (int): Visualization frequency in epochs.\n",
    "    \"\"\"\n",
    "    # Device for training\n",
    "    DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  # For MacBook with Apple Silicon\n",
    "\n",
    "    # Training parameters\n",
    "    EPOCHS = 100\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 1e-4\n",
    "    IMG_SIZE = 256  # Size for resizing images\n",
    "\n",
    "    # Model\n",
    "    ENCODER = 'se_resnet50'  # Backbone encoder architecture\n",
    "    ENCODER_WEIGHTS = 'imagenet'  # Pre-trained weights\n",
    "    CLASSES = ['entire', 'numbers', 'hands', 'contour']  # Classes to segment\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "    # Paths\n",
    "    DATASET_PATH = \"/Users/diegotovar/Pictures/MoCA/Images Mask Files Exit/train_aumented_v2\"\n",
    "    TEST_PATH = \"/Users/diegotovar/Pictures/MoCA/Images Mask Files Exit/test\"\n",
    "    CHECKPOINT_PATH = DYNAMIC_CHECKPOINT_PATH\n",
    "\n",
    "    # Additional parameters\n",
    "    PATIENCE = 10  # For early stopping\n",
    "    VAL_FREQUENCY = 1  # How often to validate (in epochs)\n",
    "    SAVE_FREQUENCY = 5  # How often to save models (in epochs)\n",
    "    VISUALIZATION_FREQUENCY = 5  # How often to visualize results (in epochs)\n",
    "\n",
    "# Verify available device\n",
    "print(f\"Training device: {Config.DEVICE}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\" if hasattr(torch.backends, 'mps') else \"MPS not available\")\n",
    "print(f\"Checkpoint path created: {Config.CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfc3d9",
   "metadata": {},
   "source": [
    "## Data Verification Function\n",
    "\n",
    "This code implements a thorough verification process for the training dataset, checking that all required files exist and have the expected format. It also visualizes sample masks to help identify potential issues in the segmentation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_data():\n",
    "    \"\"\"\n",
    "    Verifies that the dataset exists and has the correct format.\n",
    "    \n",
    "    This function checks:\n",
    "    - If the dataset directory exists\n",
    "    - If there are folders within the dataset directory\n",
    "    - If each folder contains a background image\n",
    "    - If each folder contains the required mask files for each class\n",
    "    - If mask files have proper alpha channels and content\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if verification succeeds, False otherwise.\n",
    "    \n",
    "    Note:\n",
    "        Sample masks will be saved to the checkpoint directory for visual inspection.\n",
    "        For the first sample folder, it will save visualizations of all class masks.\n",
    "    \"\"\"\n",
    "    print(f\"Verifying data in {Config.DATASET_PATH}\")\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(Config.DATASET_PATH):\n",
    "        print(f\"ERROR: Directory {Config.DATASET_PATH} does not exist.\")\n",
    "        return False\n",
    "    \n",
    "    # Get folders\n",
    "    folders = [f for f in os.listdir(Config.DATASET_PATH) \n",
    "               if os.path.isdir(os.path.join(Config.DATASET_PATH, f))]\n",
    "    \n",
    "    if len(folders) == 0:\n",
    "        print(f\"ERROR: No folders found in {Config.DATASET_PATH}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Found {len(folders)} folders.\")\n",
    "    \n",
    "    # Verify structure in some random folders\n",
    "    sample_folders = random.sample(folders, min(5, len(folders)))\n",
    "    \n",
    "    for folder in sample_folders:\n",
    "        folder_path = os.path.join(Config.DATASET_PATH, folder)\n",
    "        print(f\"\\nVerifying folder: {folder}\")\n",
    "        \n",
    "        # Look for background image\n",
    "        background_files = glob(os.path.join(folder_path, f\"{folder}_[Bb]ackground.*\"))\n",
    "        if not background_files:\n",
    "            print(f\"  WARNING: No background image found in {folder}\")\n",
    "            continue\n",
    "        \n",
    "        background_path = background_files[0]\n",
    "        print(f\"  Background image: {os.path.basename(background_path)}\")\n",
    "        \n",
    "        # Verify masks\n",
    "        for class_name in Config.CLASSES:\n",
    "            mask_path = os.path.join(folder_path, f\"{folder}_{class_name}.*\")\n",
    "            mask_files = glob(mask_path)\n",
    "            \n",
    "            if not mask_files:\n",
    "                print(f\"  WARNING: No mask found for {class_name}\")\n",
    "                continue\n",
    "            \n",
    "            mask_file = mask_files[0]\n",
    "            # Read mask with alpha channel (transparency)\n",
    "            mask = cv2.imread(mask_file, cv2.IMREAD_UNCHANGED)\n",
    "            \n",
    "            if mask is None:\n",
    "                print(f\"  ERROR: Could not read mask {os.path.basename(mask_file)}\")\n",
    "                continue\n",
    "            \n",
    "            # Check if mask has alpha channel\n",
    "            has_alpha = mask.shape[-1] == 4\n",
    "            alpha_channel = mask[:,:,3] if has_alpha else None\n",
    "            \n",
    "            # Verify mask has data\n",
    "            mask_sum = np.sum(alpha_channel) if has_alpha else np.sum(mask)\n",
    "            mask_max = np.max(alpha_channel) if has_alpha else np.max(mask)\n",
    "            \n",
    "            print(f\"  Mask {class_name}: sum={mask_sum}, max={mask_max}, \"\n",
    "                  f\"shape={mask.shape}, has_alpha={has_alpha}\")\n",
    "            \n",
    "            # Visualize the first mask of each type for the first folder\n",
    "            if folder == sample_folders[0]:\n",
    "                plt.figure(figsize=(5, 5))\n",
    "                # If it has alpha channel, show only that channel\n",
    "                if has_alpha:\n",
    "                    plt.imshow(alpha_channel, cmap='gray')\n",
    "                else:\n",
    "                    plt.imshow(cv2.cvtColor(mask, cv2.COLOR_BGR2RGB))\n",
    "                plt.title(f'Mask {class_name}')\n",
    "                plt.colorbar()\n",
    "                plt.savefig(os.path.join(Config.CHECKPOINT_PATH, f'sample_mask_{class_name}.png'))\n",
    "                plt.close()\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run verification\n",
    "verify_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb26ca",
   "metadata": {},
   "source": [
    "## Path Collection for Training Data\n",
    "\n",
    "This code retrieves and validates all image and mask paths from the dataset directory, keeping track of images that have incomplete mask sets while ensuring all files can be properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_paths():\n",
    "    \"\"\"\n",
    "    Gets paths for all images and their corresponding masks, even if they don't have all masks.\n",
    "    \n",
    "    This function:\n",
    "    - Scans all folders in the dataset directory\n",
    "    - Finds background images and corresponding masks\n",
    "    - Verifies that the images and masks can be read\n",
    "    - Tracks images with incomplete mask sets\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing image paths and available mask paths\n",
    "    \"\"\"\n",
    "    img_folders = sorted([f for f in os.listdir(Config.DATASET_PATH) \n",
    "                          if os.path.isdir(os.path.join(Config.DATASET_PATH, f))])\n",
    "    \n",
    "    img_data = []\n",
    "    incomplete_mask_count = 0\n",
    "    \n",
    "    for folder in tqdm(img_folders, desc=\"Loading data\"):\n",
    "        folder_path = os.path.join(Config.DATASET_PATH, folder)\n",
    "        \n",
    "        # Look for background image (may have different extensions and case)\n",
    "        background_files = glob(os.path.join(folder_path, f\"{folder}_[Bb]ackground.*\"))\n",
    "        \n",
    "        if background_files:\n",
    "            background_path = background_files[0]\n",
    "            \n",
    "            # Verify image can be read\n",
    "            img = cv2.imread(background_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Check available masks for each class\n",
    "            masks = {}\n",
    "            has_at_least_one_mask = False\n",
    "            \n",
    "            for class_name in Config.CLASSES:\n",
    "                mask_files = glob(os.path.join(folder_path, f\"{folder}_{class_name}.*\"))\n",
    "                \n",
    "                if mask_files:\n",
    "                    mask_path = mask_files[0]\n",
    "                    # Verify mask can be read\n",
    "                    mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "                    if mask is not None:\n",
    "                        masks[class_name] = mask_path\n",
    "                        has_at_least_one_mask = True\n",
    "            \n",
    "            # Discard if it doesn't have at least one mask\n",
    "            if not has_at_least_one_mask:\n",
    "                continue\n",
    "                \n",
    "            # Increment counter if it doesn't have all masks\n",
    "            if len(masks) < len(Config.CLASSES):\n",
    "                incomplete_mask_count += 1\n",
    "            \n",
    "            img_data.append({\n",
    "                'img_path': background_path,\n",
    "                'masks': masks\n",
    "            })\n",
    "    \n",
    "    print(f\"Total images found: {len(img_data)}\")\n",
    "    print(f\"Images with incomplete masks: {incomplete_mask_count}\")\n",
    "    \n",
    "    return img_data\n",
    "\n",
    "# Get image paths\n",
    "train_img_data = get_img_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420f151",
   "metadata": {},
   "source": [
    "## Dataset Creation and Data Processing Pipeline\n",
    "\n",
    "This code defines a custom dataset class for clock segmentation with special handling for thin lines and transparency. It also implements data augmentation techniques and creates the training and validation data loaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca278057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClockSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for clock segmentation with enhancements for thin lines and transparency support.\n",
    "    \n",
    "    This dataset handles multiple mask classes and includes special processing for \n",
    "    thin elements like contours and clock hands.\n",
    "    \n",
    "    Args:\n",
    "        img_data (list): List of dictionaries containing image and mask paths.\n",
    "        transform (albumentations.Compose, optional): Transformations to apply. Defaults to None.\n",
    "        enhance_masks (bool, optional): Whether to enhance thin lines. Defaults to True.\n",
    "        dilation_kernel_size (int, optional): Size of dilation kernel. Defaults to 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_data, transform=None, enhance_masks=True, dilation_kernel_size=3):\n",
    "        self.img_data = img_data\n",
    "        self.transform = transform\n",
    "        self.enhance_masks = enhance_masks\n",
    "        self.dilation_kernel_size = dilation_kernel_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_data)\n",
    "    \n",
    "    def enhance_thin_lines(self, mask):\n",
    "        \"\"\"\n",
    "        Enhances thin lines in the mask through dilation and refinement.\n",
    "        \n",
    "        Args:\n",
    "            mask (numpy.ndarray): Input binary mask.\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Enhanced binary mask with thicker lines.\n",
    "        \"\"\"\n",
    "        # Create kernel for dilation\n",
    "        kernel = np.ones((self.dilation_kernel_size, self.dilation_kernel_size), np.uint8)\n",
    "        \n",
    "        # Dilate mask to thicken lines\n",
    "        dilated = cv2.dilate(mask, kernel, iterations=1)\n",
    "        \n",
    "        # Optional: Smooth edges with a very light gaussian filter\n",
    "        # This helps reduce noise while maintaining structure\n",
    "        smoothed = cv2.GaussianBlur(dilated, (3, 3), 0.5)\n",
    "        \n",
    "        # Re-binarize to have a clear mask\n",
    "        _, enhanced = cv2.threshold(smoothed, 0.5, 1.0, cv2.THRESH_BINARY)\n",
    "        \n",
    "        return enhanced\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.img_data[idx]\n",
    "        img_path = sample['img_path']\n",
    "        mask_paths = sample['masks']\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not read image: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Scale original image to maintain consistent proportions with masks\n",
    "        original_height, original_width = image.shape[:2]\n",
    "        \n",
    "        # Load masks\n",
    "        masks = []\n",
    "        \n",
    "        for class_name in Config.CLASSES:\n",
    "            if class_name in mask_paths and os.path.exists(mask_paths[class_name]):\n",
    "                mask_path = mask_paths[class_name]\n",
    "                # Read with alpha channel\n",
    "                mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "                \n",
    "                if mask is None:\n",
    "                    # If mask can't be read, use an empty one\n",
    "                    mask = np.zeros((original_height, original_width), dtype=np.uint8)\n",
    "                else:\n",
    "                    # Resize to original image dimensions if different\n",
    "                    if mask.shape[:2] != (original_height, original_width):\n",
    "                        mask = cv2.resize(mask, (original_width, original_height), \n",
    "                                         interpolation=cv2.INTER_NEAREST)\n",
    "                    \n",
    "                    # Extract alpha channel as mask if it exists\n",
    "                    if mask.shape[-1] == 4:  # RGBA\n",
    "                        alpha_channel = mask[:, :, 3]\n",
    "                        # Binarize alpha channel\n",
    "                        _, binary_mask = cv2.threshold(alpha_channel, 127, 255, cv2.THRESH_BINARY)\n",
    "                        mask = binary_mask\n",
    "                    else:  # No alpha channel, convert to grayscale\n",
    "                        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "                        _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "                    \n",
    "                    # Normalize mask to values of 0 and 1\n",
    "                    mask = mask / 255.0\n",
    "                    \n",
    "                    # Apply enhancement for thin lines if enabled\n",
    "                    if self.enhance_masks and class_name in ['contour', 'hands']:  # Classes with thin lines\n",
    "                        mask = self.enhance_thin_lines(mask)\n",
    "            else:\n",
    "                # If mask doesn't exist for this class, use an empty one\n",
    "                mask = np.zeros((original_height, original_width), dtype=np.float32)\n",
    "            \n",
    "            masks.append(mask)\n",
    "        \n",
    "        # Create a single mask with channels for each class\n",
    "        multi_mask = np.stack(masks, axis=-1).astype(np.float32)\n",
    "        \n",
    "        # Apply transformations if they exist\n",
    "        if self.transform:\n",
    "            try:\n",
    "                augmented = self.transform(image=image, mask=multi_mask)\n",
    "                image = augmented['image']\n",
    "                multi_mask = augmented['mask']\n",
    "            except Exception as e:\n",
    "                print(f\"Error during transformation: {e}\")\n",
    "                print(f\"Image details: {img_path}, shape={image.shape}\")\n",
    "                raise\n",
    "            \n",
    "        # Ensure tensors are in the correct format\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1))\n",
    "            \n",
    "        if not isinstance(multi_mask, torch.Tensor):\n",
    "            multi_mask = torch.from_numpy(multi_mask.transpose(2, 0, 1))\n",
    "        elif multi_mask.dim() == 3 and multi_mask.shape[0] != Config.NUM_CLASSES:\n",
    "            multi_mask = multi_mask.permute(2, 0, 1)\n",
    "        \n",
    "        return image, multi_mask\n",
    "\n",
    "\n",
    "def ensure_binary_mask(mask, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Ensures that the mask remains binary (0 or 1) after transformations.\n",
    "    \n",
    "    Args:\n",
    "        mask (numpy.ndarray): Input mask that may have non-binary values.\n",
    "        threshold (float, optional): Threshold for binarization. Defaults to 0.5.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Binary mask with values 0 or 1.\n",
    "    \"\"\"\n",
    "    return (mask > threshold).astype(np.float32)\n",
    "\n",
    "\n",
    "def get_transforms(phase):\n",
    "    \"\"\"\n",
    "    Defines enhanced transformations that preserve thin lines.\n",
    "    \n",
    "    Args:\n",
    "        phase (str): Either 'train' or another value (for validation/testing).\n",
    "        \n",
    "    Returns:\n",
    "        albumentations.Compose: Composition of transformations.\n",
    "    \"\"\"\n",
    "    if phase == 'train':\n",
    "        return A.Compose([\n",
    "            # Resize with interpolation more suitable for binary masks\n",
    "            A.Resize(Config.IMG_SIZE, Config.IMG_SIZE, interpolation=cv2.INTER_NEAREST),\n",
    "            \n",
    "            # Transformations that maintain thin line structure\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "            ], p=0.5),\n",
    "            \n",
    "            # Limited noise to avoid distorting thin masks\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(var_limit=(10, 30), p=0.5),\n",
    "                A.GaussianBlur(blur_limit=3, p=0.5),\n",
    "            ], p=0.3),\n",
    "            \n",
    "            # Gentle spatial transformations\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, \n",
    "                              interpolation=cv2.INTER_NEAREST, border_mode=cv2.BORDER_CONSTANT, p=0.5),\n",
    "            \n",
    "            # Lambda to ensure binary values in masks\n",
    "            A.Lambda(mask=lambda x, **kwargs: (x > 0.5).astype(np.float32)),\n",
    "            \n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:  # valid or test\n",
    "        return A.Compose([\n",
    "            A.Resize(Config.IMG_SIZE, Config.IMG_SIZE, interpolation=cv2.INTER_NEAREST),\n",
    "            \n",
    "            # Also here for consistency\n",
    "            A.Lambda(mask=lambda x, **kwargs: (x > 0.5).astype(np.float32)),\n",
    "            \n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "# Split into training and validation sets (20% for validation)\n",
    "train_indices, valid_indices = train_test_split(\n",
    "    range(len(train_img_data)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create subsets\n",
    "train_data = [train_img_data[i] for i in train_indices]\n",
    "valid_data = [train_img_data[i] for i in valid_indices]\n",
    "\n",
    "print(f\"Training images: {len(train_data)}\")\n",
    "print(f\"Validation images: {len(valid_data)}\")\n",
    "\n",
    "# Create datasets with enhancements for thin lines\n",
    "train_dataset = ClockSegmentationDataset(\n",
    "    train_data, \n",
    "    transform=get_transforms('train'),\n",
    "    enhance_masks=True,\n",
    "    dilation_kernel_size=3  # For thick lines like contours\n",
    ")\n",
    "valid_dataset = ClockSegmentationDataset(\n",
    "    valid_data, \n",
    "    transform=get_transforms('valid'),\n",
    "    enhance_masks=True,\n",
    "    dilation_kernel_size=3\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=Config.BATCH_SIZE, \n",
    "    shuffle=True, num_workers=0, drop_last=True)\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=Config.BATCH_SIZE, \n",
    "    shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcdfe5d",
   "metadata": {},
   "source": [
    "## Mask Visualization and Enhancement Analysis\n",
    "\n",
    "This code implements detailed visualization functions to inspect and compare different processing techniques for segmentation masks, with special focus on thin line enhancement and contour detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mask_enhancements(dataset, idx=0, show_preprocessing=True):\n",
    "    \"\"\"\n",
    "    Visualizes a sample from the dataset with comparison between original and enhanced masks.\n",
    "    \n",
    "    This function shows the original image alongside its masks in different processing stages.\n",
    "    \n",
    "    Args:\n",
    "        dataset (ClockSegmentationDataset): The dataset to visualize from.\n",
    "        idx (int, optional): Index of the sample to visualize. Defaults to 0.\n",
    "        show_preprocessing (bool, optional): Whether to show intermediate preprocessing \n",
    "            steps. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (image, original_mask, enhanced_mask) as numpy arrays.\n",
    "    \"\"\"\n",
    "    # Get original image and masks (without enhancements)\n",
    "    temp_enhance_setting = dataset.enhance_masks\n",
    "    dataset.enhance_masks = False\n",
    "    image_orig, mask_orig = dataset[idx]\n",
    "    \n",
    "    # Restore configuration to get enhanced masks\n",
    "    dataset.enhance_masks = temp_enhance_setting\n",
    "    image, mask_enhanced = dataset[idx]\n",
    "    \n",
    "    img_path = dataset.img_data[idx]['img_path']\n",
    "    img_name = os.path.basename(img_path).split('_Background')[0]\n",
    "    \n",
    "    # Convert tensors to numpy arrays for visualization\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        # Denormalize image\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1)\n",
    "        \n",
    "        image = image * std + mean\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        image = np.clip(image, 0, 1)\n",
    "        \n",
    "        # Convert masks to numpy\n",
    "        if isinstance(mask_orig, torch.Tensor):\n",
    "            mask_orig = mask_orig.numpy()\n",
    "        if isinstance(mask_enhanced, torch.Tensor):\n",
    "            mask_enhanced = mask_enhanced.numpy()\n",
    "    \n",
    "    # Prepare visualization in two rows\n",
    "    if show_preprocessing:\n",
    "        fig, axes = plt.subplots(3, Config.NUM_CLASSES + 1, figsize=(20, 12))\n",
    "        fig.suptitle(f\"Sample {idx+1}: {img_name} - Mask Comparison\", fontsize=16)\n",
    "        \n",
    "        # First row: Original image and dilated masks\n",
    "        axes[0, 0].imshow(image)\n",
    "        axes[0, 0].set_title('Original Image')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Second row: Original masks\n",
    "        for i, class_name in enumerate(Config.CLASSES):\n",
    "            if mask_orig.shape[0] == len(Config.CLASSES):  # If in C,H,W format\n",
    "                axes[1, i+1].imshow(mask_orig[i], cmap='gray', vmin=0, vmax=1)\n",
    "                axes[0, i+1].imshow(cv2.dilate(mask_orig[i].astype(np.float32), \n",
    "                                             np.ones((3,3), np.uint8), iterations=1), \n",
    "                                   cmap='gray', vmin=0, vmax=1)\n",
    "            else:  # If in H,W,C format\n",
    "                axes[1, i+1].imshow(mask_orig[:,:,i], cmap='gray', vmin=0, vmax=1)\n",
    "                axes[0, i+1].imshow(cv2.dilate(mask_orig[:,:,i].astype(np.float32), \n",
    "                                             np.ones((3,3), np.uint8), iterations=1),\n",
    "                                   cmap='gray', vmin=0, vmax=1)\n",
    "            axes[0, i+1].set_title(f'Dilated mask: {class_name}')\n",
    "            axes[1, i+1].set_title(f'Original mask: {class_name}')\n",
    "            axes[0, i+1].axis('off')\n",
    "            axes[1, i+1].axis('off')\n",
    "        \n",
    "        # Third row: Enhanced masks\n",
    "        axes[1, 0].imshow(image)\n",
    "        axes[1, 0].set_title('Image for reference')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[2, 0].imshow(image)\n",
    "        axes[2, 0].set_title('Image for reference')\n",
    "        axes[2, 0].axis('off')\n",
    "        \n",
    "        for i, class_name in enumerate(Config.CLASSES):\n",
    "            if mask_enhanced.shape[0] == len(Config.CLASSES):\n",
    "                axes[2, i+1].imshow(mask_enhanced[i], cmap='gray', vmin=0, vmax=1)\n",
    "            else:\n",
    "                axes[2, i+1].imshow(mask_enhanced[:,:,i], cmap='gray', vmin=0, vmax=1)\n",
    "            axes[2, i+1].set_title(f'Enhanced mask: {class_name}')\n",
    "            axes[2, i+1].axis('off')\n",
    "    else:\n",
    "        # Simple visualization without showing intermediate steps\n",
    "        fig, axes = plt.subplots(2, Config.NUM_CLASSES + 1, figsize=(20, 8))\n",
    "        fig.suptitle(f\"Sample {idx+1}: {img_name}\", fontsize=16)\n",
    "        \n",
    "        # First row: Original image and original masks\n",
    "        axes[0, 0].imshow(image)\n",
    "        axes[0, 0].set_title('Original Image')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        for i, class_name in enumerate(Config.CLASSES):\n",
    "            if mask_orig.shape[0] == len(Config.CLASSES):\n",
    "                axes[0, i+1].imshow(mask_orig[i], cmap='gray', vmin=0, vmax=1)\n",
    "            else:\n",
    "                axes[0, i+1].imshow(mask_orig[:,:,i], cmap='gray', vmin=0, vmax=1)\n",
    "            axes[0, i+1].set_title(f'Original: {class_name}')\n",
    "            axes[0, i+1].axis('off')\n",
    "            \n",
    "            # Second row: Enhanced masks\n",
    "            if mask_enhanced.shape[0] == len(Config.CLASSES):\n",
    "                axes[1, i+1].imshow(mask_enhanced[i], cmap='gray', vmin=0, vmax=1)\n",
    "            else:\n",
    "                axes[1, i+1].imshow(mask_enhanced[:,:,i], cmap='gray', vmin=0, vmax=1)\n",
    "            axes[1, i+1].set_title(f'Enhanced: {class_name}')\n",
    "            axes[1, i+1].axis('off')\n",
    "        \n",
    "        axes[1, 0].imshow(image)\n",
    "        axes[1, 0].set_title('Original Image')\n",
    "        axes[1, 0].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return image, mask_orig, mask_enhanced\n",
    "\n",
    "def compare_contours(dataset, idx=0):\n",
    "    \"\"\"\n",
    "    Specialized function to analyze contour detection in masks.\n",
    "    \n",
    "    This function applies different edge detection and morphological operations\n",
    "    to analyze and enhance contours in the segmentation masks.\n",
    "    \n",
    "    Args:\n",
    "        dataset (ClockSegmentationDataset): The dataset to visualize from.\n",
    "        idx (int, optional): Index of the sample to analyze. Defaults to 0.\n",
    "    \"\"\"\n",
    "    # Get the sample\n",
    "    image, mask = dataset[idx]\n",
    "    img_path = dataset.img_data[idx]['img_path']\n",
    "    img_name = os.path.basename(img_path).split('_Background')[0]\n",
    "    \n",
    "    # Convert to numpy for processing\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        # Denormalize image\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1)\n",
    "        \n",
    "        image = image * std + mean\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        image = np.clip(image, 0, 1)\n",
    "        \n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask = mask.numpy()\n",
    "    \n",
    "    # Extract contour mask (assuming it's a specific class)\n",
    "    contour_idx = Config.CLASSES.index('contour')\n",
    "    \n",
    "    if mask.shape[0] == len(Config.CLASSES):  # C,H,W format\n",
    "        contour_mask = mask[contour_idx]\n",
    "    else:  # H,W,C format\n",
    "        contour_mask = mask[:,:,contour_idx]\n",
    "    \n",
    "    # Convert for processing with OpenCV\n",
    "    contour_mask_cv = (contour_mask * 255).astype(np.uint8)\n",
    "    \n",
    "    # Different edge detection methods\n",
    "    edges_canny = cv2.Canny(contour_mask_cv, 50, 150)\n",
    "    \n",
    "    # Gentle dilation\n",
    "    kernel1 = np.ones((3, 3), np.uint8)\n",
    "    dilated = cv2.dilate(contour_mask_cv, kernel1, iterations=1)\n",
    "    \n",
    "    # Stronger dilation\n",
    "    kernel2 = np.ones((5, 5), np.uint8)\n",
    "    dilated_more = cv2.dilate(contour_mask_cv, kernel2, iterations=1)\n",
    "    \n",
    "    # Morphological filter to enhance lines\n",
    "    kernel_line = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 5))\n",
    "    morph_line = cv2.morphologyEx(contour_mask_cv, cv2.MORPH_CLOSE, kernel_line)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle(f\"Contour Analysis - {img_name}\", fontsize=16)\n",
    "    \n",
    "    axes[0, 0].imshow(image)\n",
    "    axes[0, 0].set_title('Original Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(contour_mask, cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0, 1].set_title('Original Mask')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(edges_canny, cmap='gray')\n",
    "    axes[0, 2].set_title('Canny Edge Detection')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(dilated, cmap='gray')\n",
    "    axes[1, 0].set_title('Gentle Dilation')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(dilated_more, cmap='gray')\n",
    "    axes[1, 1].set_title('Strong Dilation')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    axes[1, 2].imshow(morph_line, cmap='gray')\n",
    "    axes[1, 2].set_title('Morphological Filter')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some random samples with applied enhancements\n",
    "print(\"\\nSamples with enhanced thin line processing:\")\n",
    "for i in range(3):\n",
    "    idx = random.randint(0, len(valid_dataset)-1)\n",
    "    img_name = os.path.basename(valid_dataset.img_data[idx]['img_path']).split('_Background')[0]\n",
    "    print(f\"\\nSample {i+1}: {img_name}\")\n",
    "    visualize_mask_enhancements(valid_dataset, idx=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c82f3",
   "metadata": {},
   "source": [
    "## Thin Line Processing and Method Comparison\n",
    "\n",
    "This code defines advanced techniques for enhancing thin lines in segmentation masks, particularly useful for detecting contours and hands in clock drawings. It also includes a visualization function to compare different processing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_thin_lines(mask, method='morphological', params=None):\n",
    "    \"\"\"\n",
    "    Processes thin lines in masks using different techniques.\n",
    "    \n",
    "    This function implements various computer vision techniques to enhance thin lines\n",
    "    in binary masks for improved segmentation performance.\n",
    "    \n",
    "    Args:\n",
    "        mask (numpy.ndarray): Original mask (values between 0-1 or 0-255).\n",
    "        method (str, optional): Method to use ('dilation', 'morphological', 'thinning', 'adaptive'). \n",
    "            Defaults to 'morphological'.\n",
    "        params (dict, optional): Specific parameters for the chosen method. \n",
    "            Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Processed mask with the same value range as the input.\n",
    "    \"\"\"\n",
    "    # Ensure mask is in 0-255 format for OpenCV\n",
    "    if mask.max() <= 1.0:\n",
    "        mask_cv = (mask * 255).astype(np.uint8)\n",
    "    else:\n",
    "        mask_cv = mask.astype(np.uint8)\n",
    "    \n",
    "    # Configure default parameters if not provided\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    if method == 'dilation':\n",
    "        # Simple dilation to thicken lines\n",
    "        kernel_size = params.get('kernel_size', 3)\n",
    "        iterations = params.get('iterations', 1)\n",
    "        \n",
    "        kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "        processed = cv2.dilate(mask_cv, kernel, iterations=iterations)\n",
    "        \n",
    "    elif method == 'morphological':\n",
    "        # More advanced morphological operations\n",
    "        kernel_size = params.get('kernel_size', 3)\n",
    "        close_iterations = params.get('close_iterations', 1)\n",
    "        open_iterations = params.get('open_iterations', 0)\n",
    "        \n",
    "        # Close first to connect nearby lines\n",
    "        kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "        processed = cv2.morphologyEx(mask_cv, cv2.MORPH_CLOSE, kernel, iterations=close_iterations)\n",
    "        \n",
    "        # Optional: open to remove noise if needed\n",
    "        if open_iterations > 0:\n",
    "            processed = cv2.morphologyEx(processed, cv2.MORPH_OPEN, kernel, iterations=open_iterations)\n",
    "            \n",
    "    elif method == 'thinning':\n",
    "        # Thin lines to ensure consistency\n",
    "        # First dilate to ensure connectivity\n",
    "        kernel_dilate = np.ones((3, 3), np.uint8)\n",
    "        dilated = cv2.dilate(mask_cv, kernel_dilate, iterations=1)\n",
    "        \n",
    "        # Zhang-Suen skeletonization algorithm\n",
    "        # Simplified implementation - for a full version use skimage.morphology.skeletonize\n",
    "        thinned = dilated.copy()\n",
    "        \n",
    "        # Iterate until no more changes\n",
    "        prev = np.zeros_like(thinned)\n",
    "        while not np.array_equal(thinned, prev):\n",
    "            prev = thinned.copy()\n",
    "            # Apply controlled erosion\n",
    "            kernel = np.ones((3, 3), np.uint8)\n",
    "            eroded = cv2.erode(thinned, kernel, iterations=1)\n",
    "            # Recover critical lines\n",
    "            thinned = cv2.bitwise_and(thinned, cv2.bitwise_not(cv2.subtract(thinned, eroded)))\n",
    "        \n",
    "        # Ensure visibility by slightly dilating the result\n",
    "        processed = cv2.dilate(thinned, np.ones((2, 2), np.uint8), iterations=1)\n",
    "        \n",
    "    elif method == 'adaptive':\n",
    "        # Adaptive method that combines different techniques\n",
    "        # First analyze mask characteristics\n",
    "        non_zero = np.count_nonzero(mask_cv)\n",
    "        total_pixels = mask_cv.size\n",
    "        density = non_zero / total_pixels\n",
    "        \n",
    "        # Apply technique according to density\n",
    "        if density < 0.01:  # Very few lines -> more dilation\n",
    "            kernel_size = 5\n",
    "            iterations = 2\n",
    "        elif density < 0.05:  # Few lines -> moderate dilation\n",
    "            kernel_size = 3\n",
    "            iterations = 1\n",
    "        else:  # Higher density -> gentle dilation\n",
    "            kernel_size = 2\n",
    "            iterations = 1\n",
    "        \n",
    "        # Apply adaptive dilation\n",
    "        kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "        processed = cv2.dilate(mask_cv, kernel, iterations=iterations)\n",
    "        \n",
    "        # Smooth edges\n",
    "        processed = cv2.GaussianBlur(processed, (3, 3), 0.5)\n",
    "        \n",
    "        # Re-binarize to maintain sharpness\n",
    "        _, processed = cv2.threshold(processed, 127, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "    else:\n",
    "        # If no valid method is specified, return the original mask\n",
    "        processed = mask_cv\n",
    "    \n",
    "    # Normalize according to input format\n",
    "    if mask.max() <= 1.0:\n",
    "        return processed / 255.0\n",
    "    else:\n",
    "        return processed\n",
    "\n",
    "def compare_methods(image_path, mask_path, methods=['original', 'dilation', 'morphological', 'adaptive']):\n",
    "    \"\"\"\n",
    "    Compares different processing methods on a single mask.\n",
    "    \n",
    "    This function loads an image and mask, applies various enhancement methods,\n",
    "    and visualizes the results side by side for comparison.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the original image.\n",
    "        mask_path (str): Path to the mask to process.\n",
    "        methods (list, optional): List of methods to apply. \n",
    "            Defaults to ['original', 'dilation', 'morphological', 'adaptive'].\n",
    "            \n",
    "    Returns:\n",
    "        dict: Dictionary containing the original and processed masks.\n",
    "    \"\"\"\n",
    "    # Load image and mask\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Load mask with alpha channel\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "    if mask is None:\n",
    "        print(f\"Could not load mask: {mask_path}\")\n",
    "        return\n",
    "    \n",
    "    # Extract alpha channel if it exists\n",
    "    if mask.shape[-1] == 4:  # RGBA\n",
    "        alpha_channel = mask[:, :, 3]\n",
    "        # Binarize alpha channel\n",
    "        _, binary_mask = cv2.threshold(alpha_channel, 127, 255, cv2.THRESH_BINARY)\n",
    "        mask = binary_mask\n",
    "    else:  # No alpha channel, convert to grayscale\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Normalize mask to 0-1\n",
    "    mask = mask / 255.0\n",
    "    \n",
    "    # Apply different methods\n",
    "    results = {'original': mask}\n",
    "    \n",
    "    for method in methods:\n",
    "        if method == 'original':\n",
    "            continue  # Already have it\n",
    "            \n",
    "        # Process with corresponding method\n",
    "        processed = process_thin_lines(mask, method=method)\n",
    "        results[method] = processed\n",
    "    \n",
    "    # Visualize results\n",
    "    n_methods = len(methods)\n",
    "    fig, axes = plt.subplots(1, n_methods + 1, figsize=(4*(n_methods + 1), 4))\n",
    "    \n",
    "    # Show original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show each method\n",
    "    for i, method in enumerate(methods):\n",
    "        axes[i+1].imshow(results[method], cmap='gray', vmin=0, vmax=1)\n",
    "        axes[i+1].set_title(f'Method: {method}')\n",
    "        axes[i+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.CHECKPOINT_PATH, 'method_comparison.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c8a18",
   "metadata": {},
   "source": [
    "## Advanced Mask Processing Comparison\n",
    "\n",
    "This code analyzes different morphological processing techniques on random validation samples to identify optimal methods for enhancing thin lines in clock drawings, especially for contours and clock hands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different methods on some random samples\n",
    "sample_indices = random.sample(range(len(valid_dataset)), min(3, len(valid_dataset)))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    print(f\"\\nAnalyzing sample {i+1}/{len(sample_indices)} (index {idx})\")\n",
    "    \n",
    "    # Sample metadata\n",
    "    img_path = valid_dataset.img_data[idx]['img_path']\n",
    "    img_name = os.path.basename(img_path).split('_Background')[0]\n",
    "    \n",
    "    print(f\"Image: {img_name}\")\n",
    "    \n",
    "    # Process contours if available\n",
    "    if 'contour' in valid_dataset.img_data[idx]['masks']:\n",
    "        contour_mask_path = valid_dataset.img_data[idx]['masks']['contour']\n",
    "        print(\"\\nComparing methods for contours:\")\n",
    "        compare_methods(img_path, contour_mask_path, \n",
    "                       methods=['original', 'dilation', 'morphological', 'adaptive'])\n",
    "        \n",
    "        # Deeper analysis for the first example\n",
    "        if i == 0:\n",
    "            compare_contours(valid_dataset, idx)\n",
    "    \n",
    "    # Process clock hands if available\n",
    "    if 'hands' in valid_dataset.img_data[idx]['masks']:\n",
    "        hands_mask_path = valid_dataset.img_data[idx]['masks']['hands']\n",
    "        print(\"\\nComparing methods for clock hands:\")\n",
    "        compare_methods(img_path, hands_mask_path, \n",
    "                       methods=['original', 'dilation', 'morphological', 'adaptive'])\n",
    "                       \n",
    "    # Visualize complete comparison\n",
    "    print(\"\\nVisualizing complete comparison of all masks:\")\n",
    "    visualize_mask_enhancements(valid_dataset, idx, show_preprocessing=True)\n",
    "\n",
    "print(\"\\nRecommendation summary:\")\n",
    "print(\"- For contours: 'adaptive' method with kernel_size=3\")\n",
    "print(\"- For clock hands: 'morphological' method with kernel_size=2, close_iterations=1\")\n",
    "print(\"- For both cases, always use INTER_NEAREST interpolation in transformations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e69c43",
   "metadata": {},
   "source": [
    "## Loss Functions and Evaluation Metrics\n",
    "\n",
    "This code defines custom loss functions and evaluation metrics optimized for multi-class segmentation tasks, particularly focused on handling thin structures in clock drawings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Dice Loss for segmentation with class weighting options.\n",
    "    \n",
    "    The Dice coefficient measures the overlap between predicted and ground truth masks.\n",
    "    This loss is particularly effective for imbalanced segmentation problems.\n",
    "    \n",
    "    Args:\n",
    "        smooth (float, optional): Small constant to prevent division by zero. Defaults to 1e-6.\n",
    "        weight (torch.Tensor, optional): Class weights tensor. Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1e-6, weight=None):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.weight = weight  # Class weights\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Flatten tensors for per-class calculation\n",
    "        batch_size = targets.size(0)\n",
    "        num_classes = targets.size(1)\n",
    "        \n",
    "        # If weights provided use them, otherwise all weights are 1\n",
    "        weight = self.weight if self.weight is not None else torch.ones(num_classes).to(inputs.device)\n",
    "        \n",
    "        # Calculate Dice for each class\n",
    "        dice_scores = []\n",
    "        for class_idx in range(num_classes):\n",
    "            inputs_class = inputs[:, class_idx, ...]\n",
    "            targets_class = targets[:, class_idx, ...]\n",
    "            \n",
    "            # Flatten\n",
    "            inputs_flat = inputs_class.view(batch_size, -1)\n",
    "            targets_flat = targets_class.view(batch_size, -1)\n",
    "            \n",
    "            # Calculate intersection and union\n",
    "            intersection = (inputs_flat * targets_flat).sum(1)\n",
    "            union = inputs_flat.sum(1) + targets_flat.sum(1)\n",
    "            \n",
    "            # Calculate Dice per instance and average over batch\n",
    "            dice_score = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "            weighted_dice = weight[class_idx] * (1 - dice_score.mean())\n",
    "            dice_scores.append(weighted_dice)\n",
    "        \n",
    "        return torch.stack(dice_scores).mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Focal Loss for segmentation with weighting support.\n",
    "    \n",
    "    Focal loss addresses class imbalance by down-weighting well-classified examples,\n",
    "    making the model focus more on difficult cases.\n",
    "    \n",
    "    Args:\n",
    "        alpha (float, optional): Balancing parameter. Defaults to 1.\n",
    "        gamma (float, optional): Focusing parameter. Defaults to 2.\n",
    "        reduction (str, optional): Reduction method ('mean', 'sum', 'none'). Defaults to 'mean'.\n",
    "        weight (torch.Tensor, optional): Class weights tensor. Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean', weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.weight = weight  # Class weights\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Calculate BCE\n",
    "        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Focal term\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_term = self.alpha * (1 - pt) ** self.gamma\n",
    "        \n",
    "        # Apply class weighting if provided\n",
    "        if self.weight is not None:\n",
    "            # Expand weights to multiply by each pixel of each class\n",
    "            weight = self.weight.view(1, -1, 1, 1).expand_as(targets)\n",
    "            focal_loss = weight * focal_term * bce_loss\n",
    "        else:\n",
    "            focal_loss = focal_term * bce_loss\n",
    "        \n",
    "        # Apply reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:  # 'none'\n",
    "            return focal_loss\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines Dice Loss and Focal Loss with configurable weighting.\n",
    "    \n",
    "    This loss function leverages the strengths of both Dice loss (for shape and overlap)\n",
    "    and Focal loss (for handling class imbalance).\n",
    "    \n",
    "    Args:\n",
    "        alpha (float, optional): Alpha parameter for Focal Loss. Defaults to 1.0.\n",
    "        gamma (float, optional): Gamma parameter for Focal Loss. Defaults to 2.0.\n",
    "        dice_weight (float, optional): Weight for Dice Loss. Defaults to 1.0.\n",
    "        focal_weight (float, optional): Weight for Focal Loss. Defaults to 1.0.\n",
    "        class_weights (list or torch.Tensor, optional): Per-class weights. Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, dice_weight=1.0, focal_weight=1.0, class_weights=None):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        # Convert weights to tensor if it's a list\n",
    "        if class_weights is not None and not isinstance(class_weights, torch.Tensor):\n",
    "            class_weights = torch.tensor(class_weights)\n",
    "            \n",
    "        self.dice = DiceLoss(weight=class_weights)\n",
    "        self.focal = FocalLoss(alpha=alpha, gamma=gamma, weight=class_weights)\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_weight = focal_weight\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        dice_loss = self.dice(inputs, targets)\n",
    "        focal_loss = self.focal(inputs, targets)\n",
    "        \n",
    "        # Combine both losses\n",
    "        return self.dice_weight * dice_loss + self.focal_weight * focal_loss\n",
    "\n",
    "def iou_score(outputs, targets, threshold=0.5, smooth=1e-5):\n",
    "    \"\"\"\n",
    "    Calculates IoU (Intersection over Union) for segmentation evaluation.\n",
    "    \n",
    "    IoU is a standard metric for evaluating the quality of segmentation masks.\n",
    "    \n",
    "    Args:\n",
    "        outputs (torch.Tensor): Model predictions.\n",
    "        targets (torch.Tensor): Ground truth masks.\n",
    "        threshold (float, optional): Threshold for binary prediction. Defaults to 0.5.\n",
    "        smooth (float, optional): Small constant to prevent division by zero. Defaults to 1e-5.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (mean_iou, class_iou) containing the mean IoU and per-class IoU values.\n",
    "    \"\"\"\n",
    "    # Convert to binary with threshold\n",
    "    outputs_bin = (outputs > threshold).float()\n",
    "    \n",
    "    # Calculate intersection and union for each batch and class\n",
    "    intersection = (outputs_bin * targets).sum(dim=(2, 3))\n",
    "    union = outputs_bin.sum(dim=(2, 3)) + targets.sum(dim=(2, 3)) - intersection\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    # IoU per class\n",
    "    class_iou = iou.mean(dim=0)\n",
    "    \n",
    "    # Global mean IoU\n",
    "    mean_iou = class_iou.mean()\n",
    "    \n",
    "    return mean_iou, class_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d0a06",
   "metadata": {},
   "source": [
    "## Model Architecture and Initialization\n",
    "\n",
    "This code defines the neural network architecture selection and initialization strategy, focusing on proper weight initialization for improved training convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cadd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "    Initializes model weights appropriately to improve convergence.\n",
    "    \n",
    "    This function applies Xavier normal initialization to convolutional and linear layers\n",
    "    and constant initialization to batch normalization layers.\n",
    "    \n",
    "    Args:\n",
    "        m (nn.Module): Module to initialize.\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "    Builds the segmentation model with improved initialization.\n",
    "    \n",
    "    This function creates a segmentation model based on the configuration settings,\n",
    "    with properly initialized weights for better training convergence.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: The constructed segmentation model.\n",
    "    \"\"\"\n",
    "    # Test with different architectures\n",
    "    model_type = \"Unet++\"  # Alternatives: \"Unet\", \"FPN\", \"PSPNet\", \"LinkNet\"\n",
    "    \n",
    "    if model_type == \"Unet\":\n",
    "        model = smp.Unet(\n",
    "            encoder_name=Config.ENCODER,\n",
    "            encoder_weights=Config.ENCODER_WEIGHTS,\n",
    "            classes=Config.NUM_CLASSES,\n",
    "            activation='sigmoid'\n",
    "        )\n",
    "    elif model_type == \"Unet++\":\n",
    "        model = smp.UnetPlusPlus(\n",
    "            encoder_name=Config.ENCODER,\n",
    "            encoder_weights=Config.ENCODER_WEIGHTS,\n",
    "            classes=Config.NUM_CLASSES,\n",
    "            activation='sigmoid'\n",
    "        )\n",
    "    elif model_type == \"Linknet\":\n",
    "        model = smp.Linknet(\n",
    "            encoder_name=Config.ENCODER,\n",
    "            encoder_weights=Config.ENCODER_WEIGHTS,\n",
    "            classes=Config.NUM_CLASSES,\n",
    "            activation='sigmoid'\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Model type not supported: {model_type}\")\n",
    "    \n",
    "    # Initialize decoder weights (encoder is already pre-trained)\n",
    "    # This is crucial to prevent the model from getting stuck predicting only zeros or ones\n",
    "    # Only apply to layers that are not part of the encoder\n",
    "    for name, module in model.named_children():\n",
    "        if name != 'encoder':\n",
    "            module.apply(init_weights)\n",
    "    \n",
    "    print(f\"Model built: {model_type} with encoder {Config.ENCODER}\")\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model()\n",
    "model.to(Config.DEVICE)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Total number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48486f0",
   "metadata": {},
   "source": [
    "## Batch Visualization for Model Evaluation\n",
    "\n",
    "This code implements a comprehensive visualization function to monitor model predictions during training, showing both combined class visualizations and individual class visualizations for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch(model, data_loader, device, epoch):\n",
    "    \"\"\"\n",
    "    Visualizes predictions on a batch for analysis.\n",
    "    \n",
    "    This function creates two visualization plots:\n",
    "    1. A comparison of original images, ground truth masks, and model predictions\n",
    "    2. A detailed view of each segmentation class for both ground truth and predictions\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        data_loader (DataLoader): DataLoader containing validation/test data.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        epoch (int): Current epoch number for labeling the saved visualizations.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    images, masks = next(iter(data_loader))\n",
    "    batch_size = min(3, images.size(0))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images[:batch_size].to(device))\n",
    "    \n",
    "    # Denormalization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(batch_size, 3, figsize=(15, 5*batch_size))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Original image\n",
    "        img = images[i].cpu().detach()\n",
    "        \n",
    "        # Check tensor shape\n",
    "        if img.dim() == 4:  # If it has shape [B, C, H, W]\n",
    "            img = img.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # Continue with denormalization\n",
    "        img = img * std.squeeze(0) + mean.squeeze(0)\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Ground truth mask (combine channels for visualization)\n",
    "        mask = masks[i].cpu().numpy()\n",
    "        mask_combined = np.zeros((mask.shape[1], mask.shape[2], 3))\n",
    "        for c in range(min(3, mask.shape[0])):  # RGB channels\n",
    "            mask_combined[:, :, c] = mask[c]\n",
    "        \n",
    "        # Prediction\n",
    "        pred = outputs[i].cpu().numpy()\n",
    "        pred_combined = np.zeros((pred.shape[1], pred.shape[2], 3))\n",
    "        for c in range(min(3, pred.shape[0])):  # RGB channels\n",
    "            pred_combined[:, :, c] = pred[c]\n",
    "        \n",
    "        # If there's only one image, axes is not a 2D array\n",
    "        if batch_size == 1:\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f'Original Image')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(mask_combined)\n",
    "            axes[1].set_title(f'Ground Truth Mask')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            axes[2].imshow(pred_combined)\n",
    "            axes[2].set_title(f'Prediction')\n",
    "            axes[2].axis('off')\n",
    "        else:\n",
    "            axes[i, 0].imshow(img)\n",
    "            axes[i, 0].set_title(f'Original Image')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(mask_combined)\n",
    "            axes[i, 1].set_title(f'Ground Truth Mask')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(pred_combined)\n",
    "            axes[i, 2].set_title(f'Prediction')\n",
    "            axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.CHECKPOINT_PATH, f'viz_epoch_{epoch+1}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Additionally, visualize each class separately for the first example\n",
    "    fig, axes = plt.subplots(2, Config.NUM_CLASSES, figsize=(4*Config.NUM_CLASSES, 8))\n",
    "    \n",
    "    for c in range(Config.NUM_CLASSES):\n",
    "        # Ground truth mask for class c\n",
    "        axes[0, c].imshow(masks[0][c].cpu().numpy(), cmap='viridis')\n",
    "        axes[0, c].set_title(f'GT: {Config.CLASSES[c]}')\n",
    "        axes[0, c].axis('off')\n",
    "        \n",
    "        # Prediction for class c\n",
    "        axes[1, c].imshow(outputs[0][c].cpu().numpy(), cmap='viridis')\n",
    "        axes[1, c].set_title(f'Pred: {Config.CLASSES[c]}')\n",
    "        axes[1, c].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.CHECKPOINT_PATH, f'viz_classes_epoch_{epoch+1}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Test the function with some data\n",
    "with torch.no_grad():\n",
    "    visualize_batch(model, valid_loader, Config.DEVICE, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90b599",
   "metadata": {},
   "source": [
    "## Training Loop Implementation\n",
    "\n",
    "This code implements a robust training procedure with advanced features like class weighting, gradient clipping, and comprehensive monitoring of training and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"\n",
    "    Trains the segmentation model with optimized parameters.\n",
    "    \n",
    "    This function implements a complete training pipeline including:\n",
    "    - Class weighting for handling imbalanced data\n",
    "    - Adaptive learning rate scheduling\n",
    "    - Gradient clipping\n",
    "    - Early stopping\n",
    "    - Periodic model checkpointing\n",
    "    - Comprehensive metrics tracking\n",
    "    - Visualization of predictions during training\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, history) - Trained model and training history dictionary\n",
    "    \"\"\"\n",
    "    # Define class weights to give more importance to thin lines\n",
    "    class_weights = torch.tensor([1.0, 1.0, 2.0, 2.0]).to(Config.DEVICE)  # Higher weight for 'hands' and 'contour'\n",
    "    \n",
    "    # Define optimizer, loss function and scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)  # Reduced learning rate\n",
    "    criterion = CombinedLoss(\n",
    "        alpha=1.0, gamma=2.0,\n",
    "        dice_weight=1.0, focal_weight=1.0,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    # For storing losses and metrics\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'valid_loss': [],\n",
    "        'valid_iou': [],\n",
    "        'class_iou': {class_name: [] for class_name in Config.CLASSES},\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "    counter = 0  # For early stopping\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{Config.EPOCHS}\")\n",
    "        \n",
    "        # ===== TRAINING =====\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "        \n",
    "        for images, masks in progress_bar:\n",
    "            images = images.to(Config.DEVICE)\n",
    "            masks = masks.to(Config.DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent explosion\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update loss and progress bar\n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Save current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # ===== VALIDATION =====\n",
    "        if (epoch + 1) % Config.VAL_FREQUENCY == 0:\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            all_ious = []\n",
    "            all_class_ious = {class_name: [] for class_name in Config.CLASSES}\n",
    "            \n",
    "            progress_bar = tqdm(valid_loader, desc=\"Validation\")\n",
    "            with torch.no_grad():\n",
    "                for images, masks in progress_bar:\n",
    "                    images = images.to(Config.DEVICE)\n",
    "                    masks = masks.to(Config.DEVICE)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    valid_loss += loss.item()\n",
    "                    \n",
    "                    # Calculate IoU\n",
    "                    mean_iou, class_ious = iou_score(outputs, masks)\n",
    "                    all_ious.append(mean_iou.item())\n",
    "                    \n",
    "                    # Store IoU per class\n",
    "                    for i, class_name in enumerate(Config.CLASSES):\n",
    "                        all_class_ious[class_name].append(class_ious[i].item())\n",
    "            \n",
    "            # Average metrics\n",
    "            valid_loss /= len(valid_loader)\n",
    "            history['valid_loss'].append(valid_loss)\n",
    "            \n",
    "            valid_iou = np.mean(all_ious)\n",
    "            history['valid_iou'].append(valid_iou)\n",
    "            \n",
    "            for class_name in Config.CLASSES:\n",
    "                class_iou = np.mean(all_class_ious[class_name])\n",
    "                history['class_iou'][class_name].append(class_iou)\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step(valid_loss)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Epoch {epoch+1}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "            print(f\"Valid IoU: {valid_iou:.4f}\")\n",
    "            for class_name in Config.CLASSES:\n",
    "                class_iou = history['class_iou'][class_name][-1]\n",
    "                print(f\"  - IoU {class_name}: {class_iou:.4f}\")\n",
    "            \n",
    "            # Visualize predictions for analysis\n",
    "            if (epoch + 1) % Config.VISUALIZATION_FREQUENCY == 0:\n",
    "                visualize_batch(model, valid_loader, Config.DEVICE, epoch)\n",
    "            \n",
    "            # Check if it's the best model\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                counter = 0\n",
    "                torch.save(model.state_dict(), \n",
    "                           os.path.join(Config.CHECKPOINT_PATH, 'best_model.pth'))\n",
    "                print(f\"New best model saved! (Loss: {valid_loss:.4f})\")\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= Config.PATIENCE:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # Save checkpoint periodically\n",
    "        if (epoch + 1) % Config.SAVE_FREQUENCY == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss if 'valid_loss' in locals() else None,\n",
    "                'valid_iou': valid_iou if 'valid_iou' in locals() else None,\n",
    "                'history': history\n",
    "            }, os.path.join(Config.CHECKPOINT_PATH, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), \n",
    "               os.path.join(Config.CHECKPOINT_PATH, 'final_model.pth'))\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['valid_loss'], label='Valid Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Evolution')\n",
    "    \n",
    "    # Global IoU plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history['valid_iou'], label='Valid IoU')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('IoU Score')\n",
    "    plt.legend()\n",
    "    plt.title('IoU Evolution')\n",
    "    \n",
    "    # Class IoU plot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for class_name in Config.CLASSES:\n",
    "        plt.plot(history['class_iou'][class_name], \n",
    "                 label=f'IoU {class_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('IoU Score')\n",
    "    plt.legend()\n",
    "    plt.title('Class IoU Evolution')\n",
    "    \n",
    "    # Learning rate plot\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history['learning_rate'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Evolution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.CHECKPOINT_PATH, 'training_history.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b17082",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This code executes the training procedure defined in the \"train_model()\" function, initiating the segmentation model training process and storing both the trained model and its training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa82d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model, history = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1822454",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "This code defines and executes an evaluation function for the trained segmentation model, measuring its performance on a dataset using multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dae429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a dataset.\n",
    "    \n",
    "    This function loads a trained model and evaluates its performance on the provided\n",
    "    dataset using loss and IoU metrics, both overall and per-class.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model weights.\n",
    "        data_loader (DataLoader): DataLoader containing the evaluation dataset.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (eval_loss, valid_iou, class_iou_values) - Loss, average IoU, and per-class IoU values.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = build_model()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Define criterion and class weights\n",
    "    class_weights = torch.tensor([1.0, 1.0, 2.0, 2.0]).to(Config.DEVICE)\n",
    "    criterion = CombinedLoss(\n",
    "        alpha=1.0, gamma=2.0,\n",
    "        dice_weight=1.0, focal_weight=1.0,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "    \n",
    "    # Metrics\n",
    "    eval_loss = 0\n",
    "    all_ious = []\n",
    "    all_class_ious = {class_name: [] for class_name in Config.CLASSES}\n",
    "    \n",
    "    # Evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Prediction\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            # Calculate IoU\n",
    "            mean_iou, class_ious = iou_score(outputs, masks)\n",
    "            all_ious.append(mean_iou.item())\n",
    "            \n",
    "            # Store IoU per class\n",
    "            for i, class_name in enumerate(Config.CLASSES):\n",
    "                all_class_ious[class_name].append(class_ious[i].item())\n",
    "    \n",
    "    # Average metrics\n",
    "    eval_loss /= len(data_loader)\n",
    "    valid_iou = np.mean(all_ious)\n",
    "    \n",
    "    class_iou_values = {}\n",
    "    for class_name in Config.CLASSES:\n",
    "        class_iou_values[class_name] = np.mean(all_class_ious[class_name])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Evaluation:\")\n",
    "    print(f\"Loss: {eval_loss:.4f}\")\n",
    "    print(f\"Average IoU: {valid_iou:.4f}\")\n",
    "    for class_name, iou in class_iou_values.items():\n",
    "        print(f\"  - IoU {class_name}: {iou:.4f}\")\n",
    "    \n",
    "    return eval_loss, valid_iou, class_iou_values\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "best_model_path = os.path.join(Config.CHECKPOINT_PATH, 'best_model.pth')\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(\"Evaluating the best model on the validation set...\")\n",
    "    eval_loss, avg_iou, class_ious = evaluate_model(best_model_path, valid_loader, Config.DEVICE)\n",
    "else:\n",
    "    print(\"Best model not found. Make sure you've trained the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f3dd60",
   "metadata": {},
   "source": [
    "## Prediction and Visualization Function\n",
    "\n",
    "This code implements a comprehensive prediction and visualization system to analyze model performance using various thresholds, and displays results with color-coded segmentation masks overlaid on the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60bffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize(model_path, image_path, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Makes a prediction and visualizes the results with different thresholds.\n",
    "    \n",
    "    This function loads a trained model, processes an input image, and creates\n",
    "    two visualization plots:\n",
    "    1. A comparison of prediction probabilities and binary masks at different thresholds\n",
    "    2. Color-coded overlay visualizations of each segmentation class\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model weights.\n",
    "        image_path (str): Path to the input image.\n",
    "        threshold (float, optional): Default threshold for binarization. Defaults to 0.5.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (pred_np, pred_binary) - Raw prediction probabilities and binary masks.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = build_model()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(Config.DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not read image: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Transform the image\n",
    "    transform = A.Compose([\n",
    "        A.Resize(Config.IMG_SIZE, Config.IMG_SIZE, interpolation=cv2.INTER_NEAREST),\n",
    "        A.Lambda(mask=lambda x, **kwargs: (x > 0.5).astype(np.float32)),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    transformed = transform(image=image)\n",
    "    image_tensor = transformed['image'].unsqueeze(0).to(Config.DEVICE)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        pred = model(image_tensor)\n",
    "    \n",
    "    # Convert to numpy and test different thresholds\n",
    "    pred_np = pred.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Create a range of thresholds to test\n",
    "    thresholds = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "    pred_binary_per_threshold = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        pred_binary = (pred_np > thresh).astype(np.uint8)\n",
    "        pred_binary_per_threshold.append(pred_binary)\n",
    "    \n",
    "    # Visualize results with different thresholds\n",
    "    fig, axes = plt.subplots(len(thresholds) + 1, Config.NUM_CLASSES + 1, \n",
    "                           figsize=(5*(Config.NUM_CLASSES + 1), 5*(len(thresholds) + 1)))\n",
    "    \n",
    "    # Overall title\n",
    "    fig.suptitle('Predictions with Different Thresholds', fontsize=20)\n",
    "    \n",
    "    # First row: Original image and probabilities\n",
    "    axes[0, 0].imshow(image)\n",
    "    axes[0, 0].set_title('Original Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    for i, class_name in enumerate(Config.CLASSES):\n",
    "        # Show probability map\n",
    "        axes[0, i+1].imshow(pred_np[i], cmap='viridis')\n",
    "        axes[0, i+1].set_title(f'Prob: {class_name}')\n",
    "        axes[0, i+1].axis('off')\n",
    "    \n",
    "    # Following rows: Binary masks with different thresholds\n",
    "    for t_idx, threshold in enumerate(thresholds):\n",
    "        axes[t_idx + 1, 0].imshow(image)\n",
    "        axes[t_idx + 1, 0].set_title(f'Threshold: {threshold}')\n",
    "        axes[t_idx + 1, 0].axis('off')\n",
    "        \n",
    "        for i, class_name in enumerate(Config.CLASSES):\n",
    "            # Show binary mask for this threshold\n",
    "            axes[t_idx + 1, i+1].imshow(pred_binary_per_threshold[t_idx][i], cmap='gray')\n",
    "            axes[t_idx + 1, i+1].set_title(f'{class_name} (threshold={threshold})')\n",
    "            axes[t_idx + 1, i+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjustment for general title\n",
    "    \n",
    "    # Save visualization\n",
    "    output_name = os.path.basename(image_path).split('.')[0]\n",
    "    plt.savefig(os.path.join(Config.CHECKPOINT_PATH, f'pred_{output_name}_thresholds.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Also visualize overlay of best predictions (threshold 0.5)\n",
    "    fig, axes = plt.subplots(1, Config.NUM_CLASSES + 1, figsize=(5*(Config.NUM_CLASSES + 1), 5))\n",
    "    \n",
    "    # Overlay on original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    pred_binary = pred_binary_per_threshold[thresholds.index(0.5)]\n",
    "    \n",
    "    # Colors for each class\n",
    "    colors = ['red', 'green', 'blue', 'yellow']\n",
    "    \n",
    "    for i, class_name in enumerate(Config.CLASSES):\n",
    "        # Create a copy of the image\n",
    "        overlay = image.copy()\n",
    "        \n",
    "        # Binary mask\n",
    "        mask = pred_binary[i]\n",
    "        \n",
    "        # Create a color map for the mask\n",
    "        colored_mask = np.zeros((mask.shape[0], mask.shape[1], 4), dtype=np.uint8)\n",
    "        \n",
    "        if colors[i] == 'red':\n",
    "            colored_mask[mask == 1] = [255, 0, 0, 128]  # RGBA semi-transparent red\n",
    "        elif colors[i] == 'green':\n",
    "            colored_mask[mask == 1] = [0, 255, 0, 128]  # RGBA semi-transparent green\n",
    "        elif colors[i] == 'blue':\n",
    "            colored_mask[mask == 1] = [0, 0, 255, 128]  # RGBA semi-transparent blue\n",
    "        elif colors[i] == 'yellow':\n",
    "            colored_mask[mask == 1] = [255, 255, 0, 128]  # RGBA semi-transparent yellow\n",
    "        \n",
    "        # Overlay the colored mask\n",
    "        axes[i+1].imshow(image)\n",
    "        axes[i+1].imshow(colored_mask, alpha=0.5)\n",
    "        axes[i+1].set_title(f'Overlay: {class_name}')\n",
    "        axes[i+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.CHECKPOINT_PATH, f'pred_{output_name}_overlay.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_np, pred_binary\n",
    "\n",
    "# Test prediction on some test images if they exist\n",
    "test_images = glob(os.path.join(Config.TEST_PATH, '*/*_Background.*'))\n",
    "if test_images:\n",
    "    num_samples = min(3, len(test_images))\n",
    "    sample_images = random.sample(test_images, num_samples)\n",
    "    \n",
    "    for img_path in sample_images:\n",
    "        print(f\"\\nPrediction for: {os.path.basename(img_path)}\")\n",
    "        pred_np, pred_binary = predict_and_visualize(\n",
    "            os.path.join(Config.CHECKPOINT_PATH, 'best_model.pth'),\n",
    "            img_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5195d54",
   "metadata": {},
   "source": [
    "## Prediction Export for Clinical Use\n",
    "\n",
    "This code implements a function to save model predictions as transparent PNG masks for clinical use, with special processing for thin structures like contours and clock hands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46059c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(model_path, output_dir, threshold=0.5, test_dir=None):\n",
    "    \"\"\"\n",
    "    Saves model predictions as transparent PNG masks.\n",
    "    \n",
    "    This function processes test images through the model and saves each class\n",
    "    prediction as a separate PNG file with transparency, optimized for clinical use\n",
    "    and further analysis.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model weights.\n",
    "        output_dir (str): Directory to save prediction masks.\n",
    "        threshold (float, optional): Threshold for binarization. Defaults to 0.5.\n",
    "        test_dir (str, optional): Directory containing test images. Defaults to None.\n",
    "    \"\"\"\n",
    "    # If test directory is not specified, use the configured one\n",
    "    if test_dir is None:\n",
    "        test_dir = Config.TEST_PATH\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load the model\n",
    "    model = build_model()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(Config.DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Find test images\n",
    "    test_images = glob(os.path.join(test_dir, '*/*_Background.*'))\n",
    "    \n",
    "    if not test_images:\n",
    "        print(\"No test images to process.\")\n",
    "        return\n",
    "    \n",
    "    # Transformation for model input\n",
    "    transform = A.Compose([\n",
    "        A.Resize(Config.IMG_SIZE, Config.IMG_SIZE, interpolation=cv2.INTER_NEAREST),\n",
    "        A.Lambda(mask=lambda x, **kwargs: (x > 0.5).astype(np.float32)),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    # Process each image\n",
    "    for img_path in tqdm(test_images, desc=\"Saving predictions\"):\n",
    "        # Base name for predictions\n",
    "        base_name = os.path.basename(img_path).split('_')[0]\n",
    "        \n",
    "        # Create directory for this set of predictions\n",
    "        image_output_dir = os.path.join(output_dir, base_name)\n",
    "        os.makedirs(image_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load the image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"Could not read image: {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Save the original image\n",
    "        cv2.imwrite(os.path.join(image_output_dir, f\"{base_name}_background.png\"), image)\n",
    "        \n",
    "        # Preprocess for the model\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        transformed = transform(image=image_rgb)\n",
    "        image_tensor = transformed['image'].unsqueeze(0).to(Config.DEVICE)\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            pred = model(image_tensor)\n",
    "        \n",
    "        # Process the prediction\n",
    "        pred_np = pred.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Resize to original size\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Save each class as a transparent RGBA image\n",
    "        for i, class_name in enumerate(Config.CLASSES):\n",
    "            # Resize and apply threshold\n",
    "            pred_class = cv2.resize(pred_np[i], (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "            pred_binary = (pred_class > threshold).astype(np.uint8) * 255\n",
    "            \n",
    "            # Apply enhancement for thin lines if it's a class with thin lines\n",
    "            if class_name in ['contour', 'hands']:\n",
    "                # Use morphological operations to enhance lines\n",
    "                if class_name == 'contour':\n",
    "                    kernel_size = 3\n",
    "                else:  # hands\n",
    "                    kernel_size = 2\n",
    "                    \n",
    "                kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "                pred_binary = cv2.morphologyEx(pred_binary, cv2.MORPH_CLOSE, kernel)\n",
    "            \n",
    "            # Create an RGBA image with transparency\n",
    "            # Use white for color (255, 255, 255) and binary value for alpha\n",
    "            rgba = np.zeros((h, w, 4), dtype=np.uint8)\n",
    "            rgba[..., 0:3] = 255  # White\n",
    "            rgba[..., 3] = pred_binary  # Alpha channel\n",
    "            \n",
    "            # Save the mask as PNG with transparency\n",
    "            output_path = os.path.join(image_output_dir, f\"{base_name}_{class_name}.png\")\n",
    "            cv2.imwrite(output_path, rgba)\n",
    "    \n",
    "    print(f\"Predictions saved to: {output_dir}\")\n",
    "\n",
    "# Save predictions\n",
    "output_predictions_dir = os.path.join(Config.CHECKPOINT_PATH, 'predictions')\n",
    "save_predictions(\n",
    "    os.path.join(Config.CHECKPOINT_PATH, 'best_model.pth'),\n",
    "    output_predictions_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8b636",
   "metadata": {},
   "source": [
    "## Error Analysis and Post-Processing\n",
    "\n",
    "This code implements detailed error analysis and post-processing techniques for segmentation predictions, providing visual feedback on model performance and enhancing the quality of predicted masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction_errors(model_path, data_loader, device, num_samples=5):\n",
    "    \"\"\"\n",
    "    Analyzes specific errors in predictions.\n",
    "    \n",
    "    This function evaluates model performance on random samples by calculating false \n",
    "    positives, false negatives, and IoU scores, with visualizations to identify error patterns.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model weights.\n",
    "        data_loader (DataLoader): DataLoader containing evaluation data.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        num_samples (int, optional): Number of random samples to analyze. Defaults to 5.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = build_model()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get some random samples\n",
    "    indices = random.sample(range(len(data_loader.dataset)), min(num_samples, len(data_loader.dataset)))\n",
    "    \n",
    "    # For each sample, calculate specific errors\n",
    "    with torch.no_grad():\n",
    "        for idx in indices:\n",
    "            # Get image and mask\n",
    "            image, mask = data_loader.dataset[idx]\n",
    "            image_tensor = image.unsqueeze(0).to(device)\n",
    "            mask_tensor = mask.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Make prediction\n",
    "            pred = model(image_tensor)\n",
    "            \n",
    "            # Calculate errors for each class\n",
    "            for c, class_name in enumerate(Config.CLASSES):\n",
    "                gt_mask = mask_tensor[0, c].cpu().numpy()\n",
    "                pred_mask = (pred[0, c] > 0.5).cpu().numpy()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                intersection = np.logical_and(gt_mask, pred_mask).sum()\n",
    "                union = np.logical_or(gt_mask, pred_mask).sum()\n",
    "                iou = intersection / union if union > 0 else 0\n",
    "                \n",
    "                # False positives and negatives\n",
    "                false_positives = np.logical_and(pred_mask, np.logical_not(gt_mask)).sum()\n",
    "                false_negatives = np.logical_and(np.logical_not(pred_mask), gt_mask).sum()\n",
    "                \n",
    "                print(f\"Sample {idx}, Class {class_name}:\")\n",
    "                print(f\"  IoU: {iou:.4f}\")\n",
    "                print(f\"  False positives: {false_positives} pixels\")\n",
    "                print(f\"  False negatives: {false_negatives} pixels\")\n",
    "                \n",
    "                # Visualize comparison\n",
    "                fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "                \n",
    "                # Denormalize image\n",
    "                img_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                img_np = std * img_np + mean\n",
    "                img_np = np.clip(img_np, 0, 1)\n",
    "                \n",
    "                # Visualize original image\n",
    "                axes[0].imshow(img_np)\n",
    "                axes[0].set_title('Original Image')\n",
    "                axes[0].axis('off')\n",
    "                \n",
    "                # Ground truth mask\n",
    "                axes[1].imshow(gt_mask, cmap='gray')\n",
    "                axes[1].set_title(f'Ground Truth: {class_name}')\n",
    "                axes[1].axis('off')\n",
    "                \n",
    "                # Prediction\n",
    "                axes[2].imshow(pred_mask, cmap='gray')\n",
    "                axes[2].set_title(f'Prediction: {class_name}')\n",
    "                axes[2].axis('off')\n",
    "                \n",
    "                # Errors: Green = false positives, Red = false negatives\n",
    "                error_map = np.zeros((pred_mask.shape[0], pred_mask.shape[1], 3))\n",
    "                error_map[np.logical_and(pred_mask, np.logical_not(gt_mask))] = [0, 1, 0]  # Green for FP\n",
    "                error_map[np.logical_and(np.logical_not(pred_mask), gt_mask)] = [1, 0, 0]  # Red for FN\n",
    "                \n",
    "                axes[3].imshow(error_map)\n",
    "                axes[3].set_title('Errors (Green: FP, Red: FN)')\n",
    "                axes[3].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "# Analyze errors in some samples\n",
    "best_model_path = os.path.join(Config.CHECKPOINT_PATH, 'best_model.pth')\n",
    "if os.path.exists(best_model_path):\n",
    "    print(\"Analyzing prediction errors...\")\n",
    "    analyze_prediction_errors(best_model_path, valid_loader, Config.DEVICE, num_samples=3)\n",
    "else:\n",
    "    print(\"Best model not found. Make sure you've trained the model first.\")\n",
    "\n",
    "def apply_post_processing(mask, class_name):\n",
    "    \"\"\"\n",
    "    Applies additional post-processing to predicted masks.\n",
    "    \n",
    "    This function uses class-specific morphological operations and filtering\n",
    "    to enhance prediction quality and remove artifacts.\n",
    "    \n",
    "    Args:\n",
    "        mask (numpy.ndarray): Input binary mask (values 0-1 or 0-255).\n",
    "        class_name (str): Class name to determine appropriate processing.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Processed mask with the same value range as the input.\n",
    "    \"\"\"\n",
    "    # Convert mask to uint8 format for OpenCV\n",
    "    mask_cv = (mask * 255).astype(np.uint8)\n",
    "    \n",
    "    if class_name == 'contour':\n",
    "        # For contours, apply morphological closing\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        processed = cv2.morphologyEx(mask_cv, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "        \n",
    "        # Remove small components (noise)\n",
    "        num_labels, labels = cv2.connectedComponents(processed)\n",
    "        min_size = 20  # Threshold to consider noise\n",
    "        \n",
    "        # Keep only components larger than threshold\n",
    "        for label in range(1, num_labels):\n",
    "            component_mask = (labels == label).astype(np.uint8)\n",
    "            if np.sum(component_mask) < min_size:\n",
    "                processed[component_mask == 1] = 0\n",
    "                \n",
    "    elif class_name == 'hands':\n",
    "        # For hands, emphasize thin lines\n",
    "        kernel_line = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 5))\n",
    "        processed = cv2.morphologyEx(mask_cv, cv2.MORPH_CLOSE, kernel_line)\n",
    "        \n",
    "        # Also apply median filter to smooth\n",
    "        processed = cv2.medianBlur(processed, 3)\n",
    "        \n",
    "    elif class_name == 'numbers':\n",
    "        # For numbers, preserve more details\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        processed = cv2.morphologyEx(mask_cv, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "        \n",
    "    else:  # 'entire'\n",
    "        # For the entire figure, use opening to remove noise\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        processed = cv2.morphologyEx(mask_cv, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    \n",
    "    # Re-binarize to ensure values 0 and 255\n",
    "    _, processed = cv2.threshold(processed, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Normalize to 0-1 range if input was in that range\n",
    "    if mask.max() <= 1.0:\n",
    "        return processed / 255.0\n",
    "    else:\n",
    "        return processed\n",
    "\n",
    "def show_post_processing_comparison(model_path, image_path):\n",
    "    \"\"\"\n",
    "    Shows a comparison between original and post-processed predictions.\n",
    "    \n",
    "    This function visualizes the effect of class-specific post-processing\n",
    "    techniques on model predictions.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model weights.\n",
    "        image_path (str): Path to the input image.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (pred_binary, processed_masks) - Original binary predictions and\n",
    "            post-processed masks.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = build_model()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(Config.DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not read image: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Transform the image\n",
    "    transform = A.Compose([\n",
    "        A.Resize(Config.IMG_SIZE, Config.IMG_SIZE, interpolation=cv2.INTER_NEAREST),\n",
    "        A.Lambda(mask=lambda x, **kwargs: (x > 0.5).astype(np.float32)),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    transformed = transform(image=image)\n",
    "    image_tensor = transformed['image'].unsqueeze(0).to(Config.DEVICE)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        pred = model(image_tensor)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    pred_np = pred.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Apply threshold for binary predictions\n",
    "    pred_binary = (pred_np > 0.5).astype(np.float32)\n",
    "    \n",
    "    # Apply post-processing\n",
    "    processed_masks = []\n",
    "    for i, class_name in enumerate(Config.CLASSES):\n",
    "        processed = apply_post_processing(pred_binary[i], class_name)\n",
    "        processed_masks.append(processed)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(2, Config.NUM_CLASSES, figsize=(20, 10))\n",
    "    \n",
    "    for i, class_name in enumerate(Config.CLASSES):\n",
    "        # Original prediction\n",
    "        axes[0, i].imshow(pred_binary[i], cmap='gray')\n",
    "        axes[0, i].set_title(f'Original: {class_name}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Post-processed prediction\n",
    "        axes[1, i].imshow(processed_masks[i], cmap='gray')\n",
    "        axes[1, i].set_title(f'Post-processed: {class_name}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    output_name = os.path.basename(image_path).split('.')[0]\n",
    "    plt.savefig(os.path.join(Config.CHECKPOINT_PATH, f'post_processing_{output_name}.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_binary, processed_masks\n",
    "\n",
    "# Test post-processing on some images\n",
    "test_images = glob(os.path.join(Config.TEST_PATH, '*/*_Background.*'))\n",
    "if test_images:\n",
    "    num_samples = min(2, len(test_images))\n",
    "    sample_images = random.sample(test_images, num_samples)\n",
    "    \n",
    "    for img_path in sample_images:\n",
    "        print(f\"\\nPost-processing for: {os.path.basename(img_path)}\")\n",
    "        pred_binary, processed_masks = show_post_processing_comparison(\n",
    "            os.path.join(Config.CHECKPOINT_PATH, 'best_model.pth'),\n",
    "            img_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965390f2",
   "metadata": {},
   "source": [
    "## Model Report Generation\n",
    "\n",
    "This code creates a comprehensive report documenting the segmentation model's architecture, training parameters, performance metrics, and preprocessing techniques, providing valuable documentation for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_report(model_path, output_path=None, training_duration=None):\n",
    "    \"\"\"\n",
    "    Generates a text report with all relevant information about the model.\n",
    "    \n",
    "    This function creates a comprehensive document containing details about the model\n",
    "    architecture, training process, performance metrics, and preprocessing techniques.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the trained model.\n",
    "        output_path (str, optional): Path to save the report. \n",
    "            Defaults to CHECKPOINT_PATH/model_report.txt.\n",
    "        training_duration (float, optional): Duration of training in seconds, if known.\n",
    "            \n",
    "    Returns:\n",
    "        str: Content of the generated report.\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = os.path.join(Config.CHECKPOINT_PATH, 'model_report.txt')\n",
    "    \n",
    "    # Check if checkpoint exists to load additional data\n",
    "    checkpoint_exists = False\n",
    "    checkpoint_data = {}\n",
    "    \n",
    "    # Find the latest checkpoint\n",
    "    checkpoint_files = glob(os.path.join(Config.CHECKPOINT_PATH, 'checkpoint_epoch_*.pth'))\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = sorted(checkpoint_files)[-1]\n",
    "        try:\n",
    "            checkpoint_data = torch.load(latest_checkpoint, map_location='cpu')\n",
    "            checkpoint_exists = True\n",
    "        except:\n",
    "            print(f\"Could not load checkpoint {latest_checkpoint}\")\n",
    "    \n",
    "    # Check if we can load training history\n",
    "    history = {}\n",
    "    if checkpoint_exists and 'history' in checkpoint_data:\n",
    "        history = checkpoint_data['history']\n",
    "    \n",
    "    # Load the model to get information\n",
    "    model_info = {}\n",
    "    try:\n",
    "        model = build_model()\n",
    "        model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "        model_type = model.__class__.__name__\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        model_info = {\n",
    "            'model_type': model_type,\n",
    "            'num_params': num_params,\n",
    "            'trainable_params': trainable_params\n",
    "        }\n",
    "    except:\n",
    "        print(f\"Could not load model {model_path} for analysis\")\n",
    "    \n",
    "    # Generate final metrics if history exists\n",
    "    final_metrics = {}\n",
    "    if history:\n",
    "        if 'train_loss' in history and history['train_loss']:\n",
    "            final_metrics['final_train_loss'] = history['train_loss'][-1]\n",
    "        if 'valid_loss' in history and history['valid_loss']:\n",
    "            final_metrics['final_valid_loss'] = history['valid_loss'][-1]\n",
    "        if 'valid_iou' in history and history['valid_iou']:\n",
    "            final_metrics['final_valid_iou'] = history['valid_iou'][-1]\n",
    "        if 'class_iou' in history:\n",
    "            for class_name, iou_values in history['class_iou'].items():\n",
    "                if iou_values:\n",
    "                    final_metrics[f'final_iou_{class_name}'] = iou_values[-1]\n",
    "    \n",
    "    # Function to format duration\n",
    "    def format_duration(seconds):\n",
    "        if seconds is None:\n",
    "            return \"Not available\"\n",
    "        \n",
    "        hours, remainder = divmod(seconds, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        \n",
    "        if hours > 0:\n",
    "            return f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\"\n",
    "        elif minutes > 0:\n",
    "            return f\"{int(minutes)} minutes, {int(seconds)} seconds\"\n",
    "        else:\n",
    "            return f\"{int(seconds)} seconds\"\n",
    "    \n",
    "    # Function to safely format numeric values\n",
    "    def format_number(value, decimal_places=6):\n",
    "        if isinstance(value, (int, float)):\n",
    "            return f\"{value:.{decimal_places}f}\"\n",
    "        return \"Not available\"\n",
    "    \n",
    "    # Write the report\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"CLOCK DRAWING SEGMENTATION MODEL REPORT\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        # General information\n",
    "        f.write(\"GENERAL INFORMATION\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(f\"Generation date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Model path: {model_path}\\n\\n\")\n",
    "        \n",
    "        # Model architecture\n",
    "        f.write(\"MODEL ARCHITECTURE\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(f\"Model type: {model_info.get('model_type', 'Not available')}\\n\")\n",
    "        f.write(f\"Encoder: {Config.ENCODER}\\n\")\n",
    "        f.write(f\"Pre-trained weights: {Config.ENCODER_WEIGHTS}\\n\")\n",
    "        f.write(f\"Number of classes: {Config.NUM_CLASSES}\\n\")\n",
    "        f.write(f\"Classes: {', '.join(Config.CLASSES)}\\n\")\n",
    "        f.write(f\"Total parameters: {model_info.get('num_params', 'Not available'):,}\\n\")\n",
    "        f.write(f\"Trainable parameters: {model_info.get('trainable_params', 'Not available'):,}\\n\\n\")\n",
    "        \n",
    "        # Training parameters\n",
    "        f.write(\"TRAINING PARAMETERS\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(f\"Epochs: {Config.EPOCHS}\\n\")\n",
    "        \n",
    "        # Get carefully the number of completed epochs\n",
    "        epochs_completed = \"Not available\"\n",
    "        if checkpoint_exists and 'epoch' in checkpoint_data:\n",
    "            epochs_completed = checkpoint_data['epoch'] + 1\n",
    "        f.write(f\"Completed epochs: {epochs_completed}\\n\")\n",
    "        \n",
    "        f.write(f\"Batch Size: {Config.BATCH_SIZE}\\n\")\n",
    "        f.write(f\"Initial Learning Rate: {Config.LEARNING_RATE}\\n\")\n",
    "        \n",
    "        # Get carefully the final learning rate\n",
    "        lr_final = \"Not available\"\n",
    "        if checkpoint_exists and 'optimizer_state_dict' in checkpoint_data:\n",
    "            optimizer_dict = checkpoint_data['optimizer_state_dict']\n",
    "            if 'param_groups' in optimizer_dict and optimizer_dict['param_groups']:\n",
    "                if 'lr' in optimizer_dict['param_groups'][0]:\n",
    "                    lr_final = format_number(optimizer_dict['param_groups'][0]['lr'])\n",
    "        f.write(f\"Final Learning Rate: {lr_final}\\n\")\n",
    "        \n",
    "        f.write(f\"Training duration: {format_duration(training_duration)}\\n\\n\")\n",
    "        \n",
    "        # Final metrics\n",
    "        f.write(\"FINAL METRICS\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(f\"Final training loss: {format_number(final_metrics.get('final_train_loss', None))}\\n\")\n",
    "        f.write(f\"Final validation loss: {format_number(final_metrics.get('final_valid_loss', None))}\\n\")\n",
    "        f.write(f\"Average validation IoU: {format_number(final_metrics.get('final_valid_iou', None))}\\n\\n\")\n",
    "        \n",
    "        # IoU by class\n",
    "        f.write(\"IoU BY CLASS\\n\")\n",
    "        for class_name in Config.CLASSES:\n",
    "            iou_key = f'final_iou_{class_name}'\n",
    "            iou_value = final_metrics.get(iou_key, None)\n",
    "            f.write(f\"- {class_name}: {format_number(iou_value)}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Preprocessing\n",
    "        f.write(\"APPLIED PREPROCESSING\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(\"- Alpha channel extraction from transparent PNG masks\\n\")\n",
    "        f.write(\"- Strict mask binarization (values 0 or 1)\\n\")\n",
    "        f.write(\"- Thin line enhancement through controlled dilation for contours and hands\\n\")\n",
    "        f.write(\"- INTER_NEAREST interpolation for resizing with fine detail preservation\\n\")\n",
    "        f.write(\"- Image normalization with mean=(0.485, 0.456, 0.406) and std=(0.229, 0.224, 0.225)\\n\\n\")\n",
    "        \n",
    "        # Other observations\n",
    "        f.write(\"ADDITIONAL OBSERVATIONS\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(\"- Combined loss function (Dice Loss + Focal Loss) with class weighting\\n\")\n",
    "        f.write(\"- Higher weighting for thin line classes (hands, contour)\\n\")\n",
    "        f.write(\"- Xavier weight initialization to improve convergence\\n\")\n",
    "        f.write(\"- Early stopping with patience of 10 epochs\\n\")\n",
    "        f.write(\"- Class-specific post-processing to improve final results\\n\")\n",
    "        \n",
    "    print(f\"Model report generated at: {output_path}\")\n",
    "    \n",
    "    # Open the file for visualization\n",
    "    with open(output_path, 'r') as f:\n",
    "        report_content = f.read()\n",
    "    \n",
    "    return report_content\n",
    "\n",
    "# Generate the report\n",
    "best_model_path = os.path.join(Config.CHECKPOINT_PATH, 'best_model.pth')\n",
    "if os.path.exists(best_model_path):\n",
    "    # If you have information about the training duration, you can pass it here\n",
    "    # For example: training_duration = end_time - start_time\n",
    "    report = generate_model_report(best_model_path)\n",
    "    print(\"\\nReport content:\")\n",
    "    print(report)\n",
    "else:\n",
    "    print(\"Best model not found. Make sure you've trained the model first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
